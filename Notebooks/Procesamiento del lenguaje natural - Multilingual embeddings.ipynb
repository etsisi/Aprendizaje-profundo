{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Multilingual embeddings<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Author: Alberto Díaz Álvarez<br>Last update: 2023-05-23</small></i></div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the applications of Embeddings is multilingual transfer learning.\n",
    "\n",
    "Suppose you want to train an NLP model in several languages, but training data is only available in one of them. Collecting new training data for each of the target languages can be expensive, and translating all the texts you want to process even more so. However, with multilingual embeddings one can try to transfer a model from one language to another more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train an _embedding_ to be able to determine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will import the libraries that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 10:44:39.718482: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-24 10:44:40.244634: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-24 10:44:40.244655: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-24 10:44:41.374724: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-24 10:44:41.374821: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-24 10:44:41.374826: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordfreq import top_n_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also configure some parameters to adapt the graphic presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create the necessary directories in case they have not been created previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('tmp', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We are going to start directly from two embeddings extracted from the [FastText](https://github.com/facebookresearch/MUSE) library (belonging to [Facebook Research](https://research.fb.com/)).\n",
    "\n",
    "This is a repository containing FastText embeddings trained with Wikipedia for more than 30 languages aligned in a single vector space. We are going to focus on English and German languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_EN = 'tmp/wiki.en.vec'\n",
    "WIKI_DE = 'tmp/wiki.de.vec'\n",
    "\n",
    "if not os.path.exists('tmp/wiki.en.vec'):\n",
    "    print('Downloading english...', end='')\n",
    "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec', WIKI_EN)\n",
    "    print('OK')\n",
    "if not os.path.exists('tmp/wiki.de.vec'):\n",
    "    print('Downloading german...', end='')\n",
    "    urllib.request.urlretrieve('https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.de.vec', WIKI_DE)\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will train a simple sentiment analysis model with the IMDb set.\n",
    "\n",
    "After downloading the data, we will apply traditional preprocessing steps. We will work with a vocabulary of 10,000 words, cut long texts after 256 words and pad all shorter texts with padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 10000\n",
    "START_INDEX = 1\n",
    "OOV_INDEX   = 2\n",
    "INDEX_FROM  = 3\n",
    "EMBEDDING_DIM = 300\n",
    "SEQUENCE_LENGTH = 256\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(\n",
    "    num_words=VOCABULARY_SIZE,  # Size for our vocabulary,\n",
    "    start_char=START_INDEX,     # Start-of-sequence token index\n",
    "    oov_char=OOV_INDEX,         # Out-of-vocabulary token index\n",
    "    index_from=INDEX_FROM,      # Starting index for the actual words\n",
    ")\n",
    "\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    x_train,\n",
    "    padding='post',\n",
    "    maxlen=SEQUENCE_LENGTH\n",
    ")\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    x_test,\n",
    "    padding='post',\n",
    "    maxlen=SEQUENCE_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to transfer our model between languages, our embedding layer needs to project the words into a multilingual vector space. Therefore, we are going to initialize our embedding layer with (a subset of) the downloaded word embeddings.\n",
    "\n",
    "First, we need to figure out which words represent the indexes of the preprocessed IMDB data and then, we need to create an embedding where each row contains the word vector indexed by its row number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(embedding_path):\n",
    "    # Load embedding\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we load the weights and words in their respective variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(path):\n",
    "    print(f'Loading vectors from {path}')\n",
    "    embedding = []\n",
    "    word_index = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            if i > 100000:\n",
    "                break\n",
    "            word, emb = line.rstrip().split(' ', 1)\n",
    "            emb = np.fromstring(emb, sep=' ')\n",
    "            embedding.append(emb)\n",
    "            word_index[word] = len(word_index)\n",
    "\n",
    "    embedding = np.vstack(embedding)\n",
    "    return embedding, word_index\n",
    "\n",
    "en_embedding, en_embedding_word_index = load_embedding(WIKI_EN)\n",
    "de_embedding, de_embedding_word_index = load_embedding(WIKI_DE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_embedding_matrix(target_word_index, embedding_word_index, embedding, rows, cols):\n",
    "    embedding_matrix = np.zeros((rows, cols))\n",
    "    for word, index in target_word_index.items():\n",
    "        if index < rows and word in embedding_word_index: \n",
    "            embedding_matrix[index] = embedding[embedding_word_index[word]]\n",
    "    return embedding_matrix\n",
    "\n",
    "en_word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "en_word_index = {word: (index + INDEX_FROM) for word, index in en_word_index.items()}\n",
    "en_word_index['<PAD>']   = 0\n",
    "en_word_index['<START>'] = START_INDEX\n",
    "en_word_index['<UNK>']   = OOV_INDEX\n",
    "\n",
    "en_embedding_matrix = create_embedding_matrix(\n",
    "    target_word_index=en_word_index,\n",
    "    embedding_word_index=en_embedding_word_index,\n",
    "    embedding=en_embedding,\n",
    "    rows=VOCABULARY_SIZE + INDEX_FROM - 1,\n",
    "    cols=EMBEDDING_DIM\n",
    ")\n",
    "en_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and training our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to create and train our model, which will be practically the same as the previous example but with the embedding changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=VOCABULARY_SIZE + INDEX_FROM - 1,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        input_length=SEQUENCE_LENGTH,\n",
    "        weights=[en_embedding_matrix],\n",
    "        trainable=False\n",
    "    ),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64), return_sequences=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['binary_accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_split=0.1, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we check how the training has evolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the model does what it is supposed to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = [\n",
    "    'the movie was amazing'.split(),\n",
    "    'i hated it'.split()\n",
    "]\n",
    "vectors = [\n",
    "    [en_word_index.get(word, OOV_INDEX) for word in comment]\n",
    "    for comment in comments\n",
    "]\n",
    "padded_vectors = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    vectors,\n",
    "    padding='post',\n",
    "    maxlen=SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "model.predict(padded_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the above model cannot be used to classify texts written in German. To transfer our model from English to German, we have to replace its English embedding with a German one.\n",
    "\n",
    "Again we are going to work with a vocabulary of 10,000 words. Unfortunately, we do not have a set of relevant German texts to embed the vocabulary in, so we will use the [wordfreq] library (https://github.com/LuminosoInsight/wordfreq). This contains word frequency information for many Western languages. In particular, `top_n_list` will give us the $n$ most frequent words in a language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_n_list('en', 10))\n",
    "print(top_n_list('de', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build our vocabulary from the 10,000 most frequent German words and use it to create a German embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_word_index = {word: idx+INDEX_FROM for idx, word in enumerate(top_n_list('de', VOCABULARY_SIZE))}\n",
    "de_word_index['<PAD>']   = 0\n",
    "de_word_index['<START>'] = START_INDEX\n",
    "de_word_index['<UNK>']   = OOV_INDEX\n",
    "\n",
    "de_embedding_matrix = create_embedding_matrix(\n",
    "    target_word_index=de_word_index,\n",
    "    embedding_word_index=de_embedding_word_index,\n",
    "    embedding=de_embedding,\n",
    "    rows=VOCABULARY_SIZE + INDEX_FROM-1,\n",
    "    cols=EMBEDDING_DIM\n",
    ")\n",
    "de_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will replace the weights of the original embedding (English) with the weights of the new embedding (German). To do this we must use the `set_weights([weight_matrix])` method of the `Embedding` class object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([de_embedding_matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as if by magic (weird magic) the model we just trained with English texts can now take German texts as input and classify them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = [\n",
    "    'Ich fand den Film sehr gut'.split(),\n",
    "    'Was für ein Chaos, ich wäre lieber zu Hause geblieben.'.split(),\n",
    "    'Es war interessant, aber ich bevorzuge das Buch'.split(),\n",
    "]\n",
    "vectors        =  [\n",
    "    [de_word_index.get(word, OOV_INDEX) for word in comment]\n",
    "    for comment in comments\n",
    "]\n",
    "padded_vectors = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    vectors,\n",
    "    padding='post',\n",
    "    maxlen=SEQUENCE_LENGTH\n",
    ")\n",
    "model.predict(padded_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilingual _embeddings_ allow us to transfer a model from one language to another. It is very useful when you need to apply the same model to several languages and you only have data in one of them.\n",
    "\n",
    "Of course, this is not without problems. The more different two languages are, the worse they will behave (we have just seen this with English and German, which are not exactly very different). Languages are not only words, but also expressions, word orders, etc. However, when two languages are linguistically similar (e.g. Spanish and Asturian), this solution is reasonably good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
