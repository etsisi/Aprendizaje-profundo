{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "# Apilando redes recurrentes<a id=\"top\"></a>\n",
    "\n",
    "<i>Última actualización: 2024-03-07</small></i></div>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las RNN apiladas (del inglés _stacked_) son un tipo de red neuronal formada por múltiples capas de RNN apiladas unas sobre otras.Se ha demostrado que las RNN apiladas mejoran el rendimiento de las RNN en diversas tareas al permitir que la red aprenda representaciones más complejas de datos secuenciales.\n",
    "\n",
    "En una RNN apilada, la salida de una capa de la RNN se pasa como entrada a la capa siguiente, creando una representación jerárquica de la secuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este cuaderno resolveremos un problema anterior con este tipo de arquitectura, viendo cómo implementar un modelo con varias capas recurrentes superpuestas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A continuación importaremos las librerías que se utilizarán a lo largo del cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También configuraremos algunos parámetros para adaptar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descarga y preproceso de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez más, utilizaremos el conjunto de datos `mnist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255, x_test / 255\n",
    "\n",
    "print(f'Training shape: {x_train.shape} input, {y_train.shape} output')\n",
    "print(f'Test shape:     {x_test.shape} input, {y_test.shape} output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo basado en múltiples capas de SRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una arquitectura multicapa de SRU es casi idéntica a una multicapa tradicional. Sin embargo, hay que tener en cuenta un concepto fundamental: **las capas recurrentes se alimentan con una secuencia de entradas, no sólo con una entrada**.\n",
    "\n",
    "Para ello, haremos uso del argumento `return_sequences=True` de las unidades recurrentes. Al activarlo, hacemos que la red devuelva la secuencia completa de salidas, no sólo la última. Haciendo esto, la capa siguiente, que requiere una secuencia, será alimentada con una secuencia. Veamos la implementación utilizando dos capas `SimpleRNN` de 10 «unidades» cada una:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(units=8, return_sequences=True, input_shape=(28, 28)),\n",
    "    tf.keras.layers.SimpleRNN(units=16, return_sequences=True),\n",
    "    tf.keras.layers.SimpleRNN(units=10),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver en el resumen que la salida de la primera capa (la que tiene `return_sequences=True`) devuelve una secuencia de 28 elementos (el tamaño de la secuencia) de dimensión 10 cada uno (su número de unidades). La segunda capa, que no hace explícito el argumento (que por defecto es `False`) devuelve sólo 10 valores, los correspondientes a la última salida de la unidad recurrente después de haberla alimentado con los 28 elementos de la secuencia.\n",
    "\n",
    "Por último, vamos a compilar el modelo creado con la función de pérdida que corresponde a este tipo de problema con un optimizador de descenso de gradiente estocástico y vamos a añadir la métrica _accuracy_ (en realidad `sparse categorical accuracy`, esto es, una versión de la primera pero preparada para problemas de clasificación multiclase para salidas _sparse_) para ver cómo evoluciona este entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['sparse_categorical_accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por último, entrenaremos nuestra red durante $10$ epochs. Sí, si con las redes recurrentes era lento, con las redes apiladas lo es aún más."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Veamos cómo ha ido el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quizá la precisión con la que ha clasificado sea mejor, pero en este problema concreto, las redes convolucionales siguen siendo la mejor opción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificación de nuevas muestras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora imprimiremos algunos de los elementos de la prueba para ver cómo se han clasificado y cuáles han sido los errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict some of the test digits\n",
    "ROWS, COLS = 5, 5\n",
    "IMAGES = ROWS * COLS\n",
    "ŷ_test = np.argmax(model.predict(x_test[:IMAGES], verbose=0), axis=1)\n",
    "# And plot them\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "for i, (x, y, ŷ) in enumerate(zip(x_test[:IMAGES], y_test[:IMAGES], ŷ_test), 1):\n",
    "    ax = fig.add_subplot(ROWS, COLS, i)\n",
    "    ax.imshow(x, cmap='Greens' if y == ŷ else 'Reds')\n",
    "    ax.set_title(f'Expected: {y}, predicted: {ŷ}')\n",
    "    ax.grid(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos mostrado cómo implementar una red neuronal recurrente apilada en Keras, en concreto un modelo RNN de dos capas con 10 unidades en cada capa. Y aunque lo hemos aplicado al problema `mnist`, hemos comprobado que no es la mejor opción (al menos en tiempo de entrenamiento).\n",
    "\n",
    "Apilar RNN puede ser una técnica muy útil para modelar datos secuenciales, ya que permite al modelo capturar dependencias más complejas entre las entradas y las salidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
