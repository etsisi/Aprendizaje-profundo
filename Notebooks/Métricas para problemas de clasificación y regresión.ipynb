{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010176d1",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "# Métricas para problemas de clasificación y regresión<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Última actualización: 2024-03-04</small></i></div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554fb61b",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35405b3c",
   "metadata": {},
   "source": [
    "En un esquema de aprendizaje supervisado, el objetivo es modelar de algún modo la relación entre las características medidas de los datos y alguna etiqueta asociada a dichos datos. De esta manera, si se logra un modelo capaz de capturar la naturaleza de dicha relación, podremos usarlo para aplicar etiquetas a datos nuevos y desconocidos.\n",
    "\n",
    "Este objetivo se da en dos tareas distintas, tanto en la definición de su arquitectura como en la medición de su rendimiento:\n",
    "\n",
    "- **Tareas de regresión**, también llamadas tareas de ajuste, en las que las etiquetas son cantidades continuas, y\n",
    "- **Tareas de clasificación**, en las que las etiquetas son categorías discretas.\n",
    "\n",
    "El célebre autor [Peter Drucker](https://es.wikipedia.org/wiki/Peter_F._Drucker) dijo una vez: «No se puede mejorar lo que no se puede medir». Aquí es donde las métricas resultan útiles: evalúan la eficacia con la que el algoritmo representa el conjunto de datos. Pero cada tarea diferente viene con su propio conjunto de métricas, y es interesante conocerlas para entender el estado de nuestro modelo predictivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f243ba3",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d6d24f",
   "metadata": {},
   "source": [
    "Propondremos soluciones a dos tipos diferentes de problemas, uno de clasificación y otro de regresión. Para ellos, también definiremos una serie de métricas para medir su rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67582fcc",
   "metadata": {},
   "source": [
    "## Bibliotecas y configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58374f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e97df",
   "metadata": {},
   "source": [
    "También configuraremos algunos parámetros para adaptar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca72f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({\"axes.grid\" : False})\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06541e93",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a697a8e",
   "metadata": {},
   "source": [
    "## Problemas de regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa76c3e",
   "metadata": {},
   "source": [
    "Esta clase de problemas intentan **predecir una salida continua a partir de un conjunto de características de entrada**. Algunos ejemplos de este tipo de modelos pueden ser la predicción del precio de la vivienda, la proyección de ventas o ingresos futuros de clientes, etcétera.\n",
    "\n",
    "En este caso vamos a crear un modelo sencillo que trata de resolver el problema del conjunto de datos [_Boston Housing Price dataset_](http://lib.stat.cmu.edu/datasets/boston) al que podemos acceder directamente a través de `keras`. Las 13 características que definen cada uno de los ejemplos están descritas en su web; estas son, por orden:\n",
    "\n",
    "- `CRIM`: Tasa de criminalidad per cápita por ciudad,\n",
    "- `ZNZ`: Ratio de suelo residencial para parcelas de más de 25K pies cuadrados,\n",
    "- `INDUS`: Proporción de superficie comercial no minorista por ciudad,\n",
    "- `CHAS` 1 si la parcela linda con el río; 0 en caso contrario,\n",
    "- `NOX` Concentración de óxido nítrico (partes por 10 millones),\n",
    "- `RM` Número medio de habitaciones por vivienda,\n",
    "- `AGE` Proporción de unidades ocupadas por sus propietarios construidas antes de 1940,\n",
    "- `DIS`: Distancias ponderadas a cinco centros de empleo de Boston,\n",
    "- `RAD` Ratio de accesibilidad radial a la autopista,\n",
    "- `TAX` Tipo del impuesto sobre bienes inmuebles de valor íntegro por 10.000\\$,\n",
    "- `PTRATIO`: Ratio alumno-profesor por ciudad,\n",
    "- `B`: $1000 \\cdot (Bk - 0.63)^2$ donde $Bk$ es la proporción de negros por ciudad,\n",
    "- `LSTAT`: \\% más bajo de la población, y\n",
    "- `MEDV`: Valor medio de las viviendas ocupadas por sus propietarios en miles de \\$.\n",
    "\n",
    "El valor a predecir será el valor medios de las viviendas en miles de \\$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffc508",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data()\n",
    "\n",
    "print(f'Training shape: {x_train.shape} input, {y_train.shape} output')\n",
    "print(f'Test shape:     {x_test.shape} input, {y_test.shape} output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d8a63",
   "metadata": {},
   "source": [
    "Introducir directamente los datos brutos en una red neuronal puede ser contraproducente debido a la variabilidad en los rangos de valores entre diferentes características. Esta variabilidad puede resultar en que valores con órdenes de magnitud más altos dominen el proceso de aprendizaje, desviando desproporcionadamente la atención del modelo hacia estas características y, por tanto, ignorando otras igualmente importantes pero con rangos de valores más bajos. Este fenómeno puede interferir significativamente en la eficacia del aprendizaje, prolongando el tiempo necesario para el entrenamiento y complicando la convergencia del modelo hacia una solución óptima, debido a problemas potenciales de desvanecimiento o explosión de gradientes.\n",
    "\n",
    "Para mitigar este problema y asegurar que todas las características contribuyan equitativamente al proceso de aprendizaje, es esencial normalizar los datos antes de su introducción en el modelo. Una técnica común para lograr esto es la normalización mediante [puntuación estándar o _z-score_](https://en.wikipedia.org/wiki/Standard_score). Este método consiste en restar la media de cada característica y dividir el resultado por su desviación estándar. Como resultado, las características ajustadas se centran alrededor de una media de $0$ y tienen una desviación estándar de $1$. Este proceso de normalización no solo equilibra la escala entre las características, sino que también asume que los datos siguen una distribución aproximadamente normal en cada característica. Si los datos no se distribuyen de manera normal, podrían ser necesarias otras técnicas de normalización o transformación para preparar adecuadamente los datos para el entrenamiento de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e42b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = (x_train - x_train.mean(axis=0)) / x_train.std(axis=0)\n",
    "x_test = (x_test - x_test.mean(axis=0)) / x_test.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b761ab19",
   "metadata": {},
   "source": [
    "Veamos cómo queda la salida del conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a71a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ed8679",
   "metadata": {},
   "source": [
    "Crearemos un modelo de dos capas ocultas de 8 neuronas cada una (activación ReLU), seguidas de una única neurona de salida (ya que vamos a predecir un único valor) y, aunque en muchos casos las salidas suelen estar normalizadas, en este caso la activación de la última neurona será lineal para no añadir complicación al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(8, activation='relu', input_shape=(13,)),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.MeanSquaredError(),\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = [tf.keras.metrics.RootMeanSquaredError()]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0787935",
   "metadata": {},
   "source": [
    "Ahora, entrenaremos el modelo para ver cómo se comporta el proceso de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0813b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=len(x_train), epochs=500, validation_split=0.1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b2015",
   "metadata": {},
   "source": [
    "Ahora veamos cómo han evolucionado las métricas durante el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d6a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a89a46",
   "metadata": {},
   "source": [
    "Buen entrenamiento. Ahora vamos a predecir las salidas del conjunto de pruebas y a ponerlas frente a frente con las reales, para poder compararlas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a23b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_test = model.predict(x_test)[:,0]\n",
    "np.column_stack((y_test, ŷ_test))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29a8dc",
   "metadata": {},
   "source": [
    "A simple vista parece que el modelo predice más o menos bien. Para ver cómo de bien, sacar conclusiones o, al menos, poder comparar entre modelos, existen diferentes métricas. Veamos algunas de las más conocidas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc02c88",
   "metadata": {},
   "source": [
    "### Error absoluto medio (MAE, del inglés _mean absolute error_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bfe8d7",
   "metadata": {},
   "source": [
    "Mide la media de las diferencias absolutas entre los valores previstos y los reales. La fórmula para calcular el MAE es\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "siendo:\n",
    "\n",
    "- $n$ el número de observaciones del conjunto de pruebas,\n",
    "- $y_i$ el valor real de la $i$-ésima observación en el conjunto de prueba, y\n",
    "- $\\hat{y}_i$ el valor previsto de la observación $i$-ésima del conjunto de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0684b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y, ŷ):\n",
    "    return np.abs(y - ŷ).mean()\n",
    "\n",
    "print(f'MAE = {mae(y_test, ŷ_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f059797",
   "metadata": {},
   "source": [
    "El MAE es una métrica útil porque es interpretable y fácil de entender. Un valor MAE más bajo indica un mejor ajuste entre los valores previstos y los reales, y un valor 0 indica un ajuste perfecto.\n",
    "\n",
    "También es menos sensible a los valores atípicos que otras métricas como el MSE, que eleva al cuadrado las diferencias entre los valores predichos y los reales, y puede verse muy influido por los valores extremos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de55e2e",
   "metadata": {},
   "source": [
    "### Error cuadrático medio (MSE, del inglés _mean squared error_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2145b4",
   "metadata": {},
   "source": [
    "Mide la media de las diferencias al cuadrado entre los valores previstos y los reales. La fórmula para calcular el MSE es\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e58e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, ŷ):\n",
    "    return ((y_test - ŷ_test) ** 2).mean()\n",
    "\n",
    "print(f'MSE = {mse(y_test, ŷ_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15633878",
   "metadata": {},
   "source": [
    "El MSE es una métrica útil porque penaliza más los errores grandes que los pequeños, debido a que eleva al cuadrado las diferencias entre los valores predichos y los reales.\n",
    "\n",
    "Sin embargo, como implica elevar los errores al cuadrado, puede estar muy influenciado por valores atípicos y no ser tan interpretable como otras métricas como MAE. Un valor de MSE más bajo indica un mejor ajuste entre los valores predichos y los reales, y un valor de 0 indica un ajuste perfecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa0c84",
   "metadata": {},
   "source": [
    "### Raíz del error cuadrático medio (RMSE, del inglés _root mean squared error_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfff6ca7",
   "metadata": {},
   "source": [
    "Mide la raíz cuadrada de la media de las diferencias al cuadrado entre los valores previstos y los reales. La fórmula para calcular el RMSE es:\n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4ade9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, ŷ):\n",
    "    return mse(y, ŷ) ** .5\n",
    "\n",
    "print(f'RMSE = {rmse(y_test, ŷ_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071fac96",
   "metadata": {},
   "source": [
    "El RMSE es una métrica útil porque tiene las mismas unidades que la variable que se predice, lo que facilita su interpretación.\n",
    "\n",
    "También es menos sensible a los valores atípicos que el MSE, porque toma la raíz cuadrada de la suma de los errores al cuadrado, lo que ayuda a \"deshacer\" el efecto de cuadratura en los errores grandes. Un valor de RMSE más bajo indica un mejor ajuste entre los valores predichos y los reales, y un valor de 0 indica un ajuste perfecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820d045",
   "metadata": {},
   "source": [
    "### Coeficiente de determinación ($R^2$))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6101d",
   "metadata": {},
   "source": [
    "Mide la proporción de la varianza de la variable dependiente que explican las variables independientes del modelo. La fórmula para calcular $R^2$ es:\n",
    "\n",
    "$$R^2(y, \\hat{y}) = 1 - \\frac{\\sum_1^{n} (y_i - \\hat{y}_i)^2}{\\sum_1^{n}(y_i - \\bar{y})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e8043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(y, ŷ):\n",
    "    return 1 - ((y - ŷ) ** 2).sum() / ((y - y.mean()) ** 2).sum()\n",
    "\n",
    "print(f'R² = {r2(y_test, ŷ_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8e3207",
   "metadata": {},
   "source": [
    "$R^2$ toma valores entre $0$ y $1$, y un valor más alto indica un mejor ajuste entre los valores predichos y los reales. Un valor de $1$ indica un ajuste perfecto, en el que toda la varianza de la variable dependiente se explica por las variables independientes, mientras que un valor de $0$ indica que el modelo no explica ninguna varianza de la variable dependiente más allá de la media.\n",
    "\n",
    "Es una métrica muy útil porque es fácil de interpretar y puede utilizarse para comparar el rendimiento de distintos modelos. Sin embargo, tiene algunas limitaciones, como no poder distinguir entre un buen ajuste y un ajuste excesivo, y verse afectado por el número de variables independientes del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37fa74e",
   "metadata": {},
   "source": [
    "## Problemas de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aec0b7",
   "metadata": {},
   "source": [
    "Como decíamos en la introducción, clasificar significa predecir etiquetas discretas. Esto nos lleva a considerar los diferentes tipos de problemas de clasificación que podemos encontrar:\n",
    "\n",
    "- **Clasificación binaria**: Las etiquetas son dos y sólo dos. Por ejemplo, si queremos predecir si un paciente tiene o no una enfermedad.\n",
    "- **Clasificación multiclase**: Las etiquetas son más de dos. Por ejemplo, si queremos predecir el tipo de tumor a partir de una imagen.\n",
    "- **Clasificación multietiqueta**: Las etiquetas son más de dos y puede haber más de una etiqueta correcta. Por ejemplo, si queremos predecir el tipo de tumor a partir de una imagen y también queremos saber si el tumor es maligno o benigno.\n",
    "\n",
    "Diferentes tipos de problemas requieren diferentes formas de abordar cómo se está produciendo el error. En esta sección veremos cómo abordar cada uno de estos problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cccd65c",
   "metadata": {},
   "source": [
    "### Clasificación binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e3553",
   "metadata": {},
   "source": [
    "En este caso intentaremos resolver un problema de clasificación binaria con un modelo sencillo para ilustrar el problema. Para ello aprovecharemos la función `make_classification` de la librería `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a3e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = sklearn.datasets.make_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_classes=2\n",
    ")\n",
    "plt.scatter(X[:,0], X[:,1], marker=\"+\", c=Y, cmap='bwr');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e20b8a",
   "metadata": {},
   "source": [
    "Ahora dividiremos el conjunto en el conjunto de entrenamiento, con el 90% de los puntos, y el conjunto de prueba, con el 10% restante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b337e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = X[:-1000,:], Y[:-1000]\n",
    "x_test, y_test = X[-1000:,:], Y[-1000:]\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_train[:,0], x_train[:,1], marker=\"+\", c=y_train, cmap='bwr');\n",
    "plt.title('Training set')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x_test[:,0], x_test[:,1], marker=\"+\", c=y_test, cmap='bwr');\n",
    "plt.title('Test set');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb832e4",
   "metadata": {},
   "source": [
    "#### El modelo de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f95be9",
   "metadata": {},
   "source": [
    "Ahora crearemos un modelo capaz de clasificar valores procedentes de esta distribución. La arquitectura es indiferente, pero lo importante es que la salida es una única neurona con función de activación sigmoide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b053640",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(2,)),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b11c18",
   "metadata": {},
   "source": [
    "Ahora entrenemos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13322dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=len(x_train), epochs=500, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f959c37",
   "metadata": {},
   "source": [
    "Veamos cómo ha evolucionado el proceso de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc9af96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fcb1c2",
   "metadata": {},
   "source": [
    "Recorramos algunas de las diferentes métricas que existen para evaluar un modelo de clasificación. Para ello, extraeremos las predicciones de nuestro modelo sobre el conjunto de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c7ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_test = np.argmax(model.predict(x_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e8b498",
   "metadata": {},
   "source": [
    "#### Matriz de confusión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d8992f",
   "metadata": {},
   "source": [
    "No es una métrica como tal, sino una tabla que se utiliza en problemas de clasificación para evaluar dónde se han cometido errores en el modelo. Se utiliza para problemas de clasificación en los que la salida puede ser de dos o más tipos de clases, aunque aquí la explicaremos para problemas de clasificación binaria (de dos clases).\n",
    "\n",
    "La idea es que las filas representan las clases reales que deberían haber sido los resultados, mientras que las columnas representan las predicciones que hemos hecho. Utilizando esta tabla es fácil identificar qué predicciones son erróneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ffc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, ŷ_test\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7362f3",
   "metadata": {},
   "source": [
    "Prácticamente todas las métricas de rendimiento en problemas de clasificación se basan en los valores de esta matriz. Antes de explicar estas métricas, conviene conocer los nombres por los que se conocen los valores de acierto y error:\n",
    "\n",
    "1. **Verdaderos positivos** ($TP$, del inglés _true positives_): Aquellos casos en los que la clase real y la predicha por el modelo son verdaderas. Por ejemplo, el caso en el que una persona tiene cáncer (`true`) y el modelo clasifica su caso como cáncer (`true`).\n",
    "2. **Verdaderos negativos** ($TN$, del inglés _true negatives_): Aquellos casos en los que la clase real y la clase predicha por el modelo son falsas. Por ejemplo, el caso en que una persona **no** tiene cáncer (`falso`) y el modelo clasifica su caso como **no** tiene cáncer (`falso`).\n",
    "3. **Falsos positivos** ($FP$, del inglés _false positives_): Aquellos casos en los que la clase real es falsa, pero el modelo la predice como verdadera. Por ejemplo, una persona que **no** tiene cáncer, pero cuyo caso el modelo predice que tiene cáncer.\n",
    "4. **Falsos negativos** ($FN$, _false negatives_): Aquellos casos en los que la clase real es verdadera, pero la predicha por el modelo es falsa. Por ejemplo, el caso de una persona que sí tiene cáncer, pero para la que el modelo predice que no lo tiene.\n",
    "\n",
    "Por supuesto, buscamos el escenario en el que haya $0$ falsos positivos y $0$ falsos negativos, pero en la vida real no es así, ya que prácticamente ningún modelo será preciso al $100\\%$. Por lo tanto, siempre habrá algún error asociado a cada modelo que utilicemos para predecir la clase real de la variable objetivo. Esto dará lugar a falsos positivos y falsos negativos, que también estarán relacionados: cuando unos disminuyan otros aumentarán y viceversa.\n",
    "\n",
    "Así que cuáles son preferibles depende del problema:\n",
    "\n",
    "1. **Minimizar los falsos negativos**: Suele ser aconsejable en casos en los que pasar por alto un caso positivo es un gran error. En el caso de la detección previa de un cáncer, es preferible cometer el error de un falso positivo (al paciente se le diagnostica un cáncer cuando no lo tiene) que un falso negativo (no se le diagnostica cuando sí lo tiene), porque en este último caso no se realizaría ningún examen más.\n",
    "2. **Minimizar los falsos positivos**: Suele ser preferible en el caso contrario. Por ejemplo, en un caso de detección de spam, suele ser preferible que un correo basura no sea detectado como tal; si en este caso minimizamos los falsos negativos, los falsos positivos aumentarían y por tanto nuestro sistema podría eliminar correos auténticos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a9f294",
   "metadata": {},
   "source": [
    "#### Exactitud (_accuracy_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83486b",
   "metadata": {},
   "source": [
    "Mide la proporción de instancias clasificadas correctamente sobre el número total de instancias del conjunto de datos. La fórmula para calcular la precisión es:\n",
    "\n",
    "$$\n",
    "Acc = \\frac{TP + TN}{TP + FP + TN + FN}\n",
    "$$\n",
    "\n",
    "Es decir, el número de casos clasificados correctamente ($TP$ y $TN$) dividido por el número total de casos ($TP$, $TN$, $FP$ y $FN$). Responde a la pregunta: **¿Con qué frecuencia acierta el modelo?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc4f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, ŷ):\n",
    "    return np.sum(y == ŷ) / len(y)\n",
    "\n",
    "print(f'Accuracy = {accuracy(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696c3f97",
   "metadata": {},
   "source": [
    "La precisión es una métrica sencilla e intuitiva, fácil de entender e interpretar. Sin embargo, puede que no sea la mejor métrica en todos los casos, sobre todo cuando las clases del conjunto de datos están desequilibradas.\n",
    "\n",
    "Por ejemplo, si estamos evaluando si se va a producir una fusión del núcleo de una central nuclear, un modelo que aprenda a decir siempre que no puede acertar el $99,9999999\\%$ de las veces; desde el punto de vista de la precisión, el modelo está bien, pero siendo objetivo no vale para nada. En estos casos, otras métricas como la precisión, el recuerdo o la puntuación F1 pueden ser más apropiadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9c4e94",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35606491",
   "metadata": {},
   "source": [
    "Mide la proporción de predicciones positivas verdaderas (es decir, el número de instancias clasificadas correctamente como positivas) de todas las predicciones positivas realizadas por el modelo (es decir, la suma de predicciones positivas verdaderas y falsas positivas). La fórmula para calcular la precisión es:\n",
    "\n",
    "$$\n",
    "Pre = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Responde a la pregunta: **De los positivos predichos, ¿qué porcentaje es realmente positivo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a483ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y, ŷ):\n",
    "    TP = np.sum((ŷ_test == 1) & (y_test == 1))\n",
    "    FP = np.sum((ŷ_test == 1) & (y_test == 0))\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "print(f'Precision = {precision(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b83482f",
   "metadata": {},
   "source": [
    "Es una métrica útil cuando queremos minimizar los falsos positivos, es decir, cuando queremos asegurarnos de que las instancias que clasificamos como positivas lo son realmente. Por ejemplo, supongamos que tenemos un modelo de clasificación binario que predice si un determinado correo electrónico es spam o no. Si el modelo predice que $100$ correos electrónicos son spam y $80$ de esas predicciones son correctas, mientras que $20$ son incorrectas, la precisión es de $0,8$, lo que significa que de todos los correos electrónicos predichos como spam por el modelo, el $80\\%$ eran realmente spam.\n",
    "\n",
    "Sin embargo, la precisión por sí sola puede no ser suficiente para evaluar el rendimiento de un modelo, y debe utilizarse junto con otras métricas como _recall_ o la puntuación F1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1608783",
   "metadata": {},
   "source": [
    "#### Recuperación (_recall_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7644f1",
   "metadata": {},
   "source": [
    "También conocida como _sensitivity_, es una métrica que mide la proporción de predicciones positivas verdaderas (es decir, el número de instancias clasificadas correctamente como positivas) de todas las instancias positivas reales del conjunto de datos. La fórmula para calcular el recuerdo es\n",
    "\n",
    "$$\n",
    "Rec = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Responde a la pregunta: **De todos los casos positivos, ¿qué porcentaje ha predicho el modelo?**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y, ŷ):\n",
    "    TP = np.sum((ŷ_test == 1) & (y_test == 1))\n",
    "    FN = np.sum((ŷ_test == 0) & (y_test == 1))\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "print(f'Recall = {recall(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd11b9c6",
   "metadata": {},
   "source": [
    "Por ejemplo, supongamos que tenemos un modelo de clasificación binaria que predice si un determinado paciente tiene cáncer. Si en el conjunto de datos hay 100 pacientes que realmente tienen la enfermedad, y el modelo identifica correctamente a 80 de ellos como positivos, mientras que $20$ no son detectados, la recuperación del modelo sería de $0,8$, lo que significa que de todos los pacientes que realmente tienen la enfermedad, el modelo identificó correctamente al $80\\%$ de ellos.\n",
    "\n",
    "Es una métrica útil cuando queremos minimizar los falsos negativos, es decir, cuando queremos asegurarnos de que todos los casos positivos se identifican correctamente como positivos. Sin embargo, el recuerdo por sí solo puede no ser suficiente para evaluar el rendimiento de un modelo, y debe utilizarse junto con otras métricas como la precisión o la puntuación F1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2d4c8",
   "metadata": {},
   "source": [
    "#### Especificidad (_specificity_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c84cfe",
   "metadata": {},
   "source": [
    "Es lo contrario de la sensibilidad, que mide la proporción de predicciones negativas verdaderas (es decir, el número de ejemplos clasificados correctamente como negativos) de todas las instancias negativas reales del conjunto de datos. La fórmula para calcular la especificidad es:\n",
    "\n",
    "$$\n",
    "Spe = \\frac{TN}{TN + FP}\n",
    "$$\n",
    "\n",
    "Responde a la pregunta: **¿Qué tan bien predice el modelo los resultados negativos?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558898f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(y, ŷ):\n",
    "    TN = np.sum((ŷ_test == 0) & (y_test == 0))\n",
    "    FP = np.sum((ŷ_test == 1) & (y_test == 0))\n",
    "    return TN / (TN + FP)\n",
    "\n",
    "print(f'Specificity = {specificity(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc74583f",
   "metadata": {},
   "source": [
    "Por ejemplo, supongamos que tenemos la misma clasificación binaria que antes. Si en el conjunto de datos hay 100 pacientes que no tienen cáncer, y el modelo identifica correctamente a $70$ de ellos como negativos, mientras que $30$ se clasifican incorrectamente como positivos, la especificidad del modelo sería de $0,7$, lo que significa que de todos los pacientes que no tienen la enfermedad, el modelo identificó correctamente al $70\\%$ de ellos.\n",
    "\n",
    "La especificidad es una métrica útil cuando queremos minimizar los falsos positivos, es decir, cuando queremos asegurarnos de que todas las instancias negativas se identifican correctamente como negativas. Sin embargo, la especificidad por sí sola puede no ser suficiente para evaluar el rendimiento de un modelo, y debe utilizarse junto con otras métricas como la sensibilidad (_recall_) o la puntuación F1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a5b0f",
   "metadata": {},
   "source": [
    "#### _F1 Score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3d84c",
   "metadata": {},
   "source": [
    "Es la media armónica de la precisión y la recuperación, y proporciona una medida equilibrada del rendimiento del modelo. La fórmula para calcular el _F1 score_ se define como:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{Pre \\cdot Rec}{Pre + Rec}\n",
    "$$\n",
    "\n",
    "Su valor oscila entre $0$ y $1$, donde 1 indica una precisión y recuperación perfectas, y 0 un rendimiento deficiente. Un _F1 score_ alta indica que el modelo tiene un buen equilibrio entre precisión y recuperación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb814b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y, ŷ):\n",
    "    pre = precision(y, ŷ)\n",
    "    rec = recall(y, ŷ)\n",
    "    return 2 * pre * rec / (pre + rec)\n",
    "\n",
    "print(f'F1 Score = {f1_score(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f05897",
   "metadata": {},
   "source": [
    "Por ejemplo, teniendo el mismo modelo de clasificación binaria que antes, si el modelo tiene una precisión de $0,8$ y una recuperación de $0,7$, la puntuación F1 del modelo sería de $0,75$, lo que significa que el modelo tiene un rendimiento equilibrado entre precisión y recuperación, con una puntuación F1 de $0,75$.\n",
    "\n",
    "El _F1 score_ es una métrica útil cuando queremos evaluar el rendimiento general de un modelo, sobre todo cuando la precisión y la recuperación son métricas importantes para el problema en cuestión. Sin embargo, puede no ser adecuado para problemas en los que la precisión o la recuperación son más importantes que la otra. En tales casos, la precisión o la recuperación pueden utilizarse como métrica única para la evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a36bf0",
   "metadata": {},
   "source": [
    "#### Curvas ROC (del inglés _receiver operating characteristic_) y AUC (del inglés _area under the curve_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43466763",
   "metadata": {},
   "source": [
    "**La ROC** es una representación gráfica de la tasa de verdaderos positivos (sensibilidad) frente a la tasa de falsos positivos (1 - especificidad) con distintos umbrales.\n",
    "\n",
    "Para crear una curva ROC, primero variamos el umbral utilizado para clasificar las instancias como positivas o negativas y, para cada valor de umbral, calculamos la tasa de verdaderos positivos y la tasa de falsos positivos. La tasa de verdaderos positivos es la proporción de instancias positivas reales que se clasifican correctamente como positivas, mientras que la tasa de falsos positivos es la proporción de instancias negativas reales que se clasifican incorrectamente como positivas.\n",
    "\n",
    "A continuación, se comparan estas tasas de verdaderos positivos con las de falsos positivos, lo que da como resultado una curva ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20342f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP_rate, TP_rate, thresholds = sklearn.metrics.roc_curve(y_test, ŷ_test)\n",
    "plt.plot(FP_rate, TP_rate)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC curve for our classification problem')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed7195",
   "metadata": {},
   "source": [
    "Una línea diagonal desde la esquina inferior izquierda hasta la esquina superior derecha del gráfico representa un clasificador aleatorio. La curva ROC ideal abraza la esquina superior izquierda del gráfico, indicando una alta tasa de verdaderos positivos y una baja tasa de falsos positivos, lo que se traduce en una alta precisión.\n",
    "\n",
    "El área bajo la curva ROC (AUC) es un resumen de un solo número del rendimiento del modelo. El AUC oscila entre $0$ y $1$, con un valor de $1$ que indica un rendimiento perfecto y un valor de $0,5$ que indica un rendimiento aleatorio. En general, un AUC más alto indica un mejor rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.roc_auc_score(y_test, ŷ_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4999123",
   "metadata": {},
   "source": [
    "La curva ROC y el AUC son herramientas útiles para evaluar y comparar el rendimiento de distintos modelos de clasificación binaria, sobre todo cuando las clases están desequilibradas, es decir, una clase es mucho más frecuente que la otra. Permiten evaluar la capacidad de un modelo para distinguir entre dos clases y **comparar diferentes modelos de clasificación de forma objetiva**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c252c94",
   "metadata": {},
   "source": [
    "### Clasificación multiclase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d87be",
   "metadata": {},
   "source": [
    "La clasificación multiclase es un tipo de problema de aprendizaje automático cuyo objetivo es predecir la categoría o etiqueta de clase de una instancia de entrada a partir de un conjunto de clases posibles. En la clasificación multiclase, hay más de dos resultados o clases posibles a las que puede pertenecer una instancia, a diferencia de la clasificación binaria, en la que sólo hay dos clases posibles.\n",
    "\n",
    "Por ejemplo, un problema de clasificación multiclase podría ser predecir la raza de un gato determinado en un conjunto de datos en el que hay tres clases posibles: «siamés», «persa» y «común europeo». Otro ejemplo podría ser predecir el género de una película a partir de un conjunto de géneros posibles: «acción», «comedia», «drama» y «ciencia ficción».\n",
    "\n",
    "En la clasificación multiclase, **cada instancia** está **asociada a una etiqueta de clase** verdadera, y el objetivo del modelo de aprendizaje automático es aprender un mapeo entre las características de entrada y la etiqueta de clase correcta. Hay varios algoritmos que pueden utilizarse para la clasificación multiclase, como los árboles de decisión, las máquinas de vectores soporte, la regresión logística y las redes neuronales.\n",
    "\n",
    "La evaluación de los modelos de clasificación multiclase suele realizarse utilizando métricas como la exactitud, la precisión, la recuperación y la puntuación F1. Ya hemos visto que la exactitud mide la proporción de predicciones correctas realizadas por el modelo, mientras que la precisión, la recuperación y la puntuación F1 proporcionan información más detallada sobre el rendimiento del modelo para cada clase.\n",
    "\n",
    "Intentaremos resolver un problema como el anterior pero en el que hay más de dos clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f9519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = sklearn.datasets.make_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_classes=4,\n",
    "    n_clusters_per_class=1,\n",
    ")\n",
    "plt.scatter(X[:,0], X[:,1], marker=\"+\", c=Y, cmap='bwr');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a105fe08",
   "metadata": {},
   "source": [
    "También como antes, extraeremos un conjunto de entrenamiento y un conjunto de prueba para evaluar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf55e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = X[:-1000,:], Y[:-1000]\n",
    "x_test, y_test = X[-1000:,:], Y[-1000:]\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_train[:,0], x_train[:,1], marker=\"+\", c=y_train, cmap='bwr');\n",
    "plt.title('Training set')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x_test[:,0], x_test[:,1], marker=\"+\", c=y_test, cmap='bwr');\n",
    "plt.title('Test set');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8841bb91",
   "metadata": {},
   "source": [
    "#### El modelo de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae133b",
   "metadata": {},
   "source": [
    "Antes de crear el modelo, tenemos que echar un vistazo a los datos de salida. En la sección anterior, teníamos dos clases, y una sola neurona de salida era suficiente para discriminar entre una y otra ($0$ para una clase y $1$ para la otra).\n",
    "\n",
    "Ahora, sin embargo, tenemos cuatro salidas en total. ¿Cómo podemos hacer que el modelo sea capaz de clasificar entre todas ellas? La respuesta es que no podemos. Necesitamos una neurona para cada clase, y cada neurona tendrá que tener un valor de salida entre $0$ y $1$. La neurona con el valor más alto será la que se active, y por tanto será la clase que se prediga.\n",
    "\n",
    "Para ello, vamos a utilizar una función de activación llamada _softmax_. Esta función de activación es una generalización de la función sigmoide, y se utiliza para calcular la probabilidad de que un ejemplo pertenezca a una clase determinada. La función softmax se define como sigue:\n",
    "\n",
    "$$\n",
    "\\sigma(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}}\n",
    "$$\n",
    "\n",
    "Siendo:\n",
    "\n",
    "- $z$ el vector de entrada,\n",
    "- $z_j$ es el valor de entrada de la neurona $j$, y\n",
    "- $K$ es el número de clases.\n",
    "\n",
    "La función softmax se aplica a cada neurona de salida, y el resultado es un vector de probabilidades que suman 1. Por lo tanto, para nuestro problema, tendremos que tener cuatro neuronas de salida, y cada una de ellas tendrá que tener un valor entre 0 y 1. La neurona con el valor más alto será la que active la capa _softmax_, y por lo tanto, será la clase que se prediga.\n",
    "\n",
    "Sabiendo esto, vamos a crear el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89388001",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(2,)),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='softmax'),\n",
    "])\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c111ee00",
   "metadata": {},
   "source": [
    "Fijémonos por un momento en el argumento de _loss_. En el ejemplo anterior hemos utilizado _binary_crossentropy_, que es la función de pérdida utilizada para problemas de clasificación binaria. En este caso, sin embargo, tenemos cuatro clases, y por lo tanto, necesitamos una función de pérdida que se adapte a este tipo de problema. En este caso, utilizaremos _categorical_crossentropy_.\n",
    "\n",
    "Estas dos funciones son las mismas para los problemas de clasificación binaria. Sin embargo, _binary_crossentropy_ es más eficiente que _categorical_crossentropy_ para problemas de clasificación binaria, entre otras cosas porque no hay que hacer ninguna codificación _one-hot_ de la salida.\n",
    "\n",
    "Pero, ¿en qué consiste esta codificación _one-hot_? Veamos la forma de las clases de nuestro conjunto de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad374ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae94e2d",
   "metadata": {},
   "source": [
    "Es sólo una salida numérica. En otras palabras, es una salida de dimensión 1, pero hemos definido la salida de nuestro modelo como de dimensión 4. Para ello tenemos que recodificar la salida de alguna manera que nos permita tener una salida de dimensión 4. Para ello tenemos que recodificar la salida de alguna manera que nos permita tener una salida de dimensión 4. Para ello utilizaremos la codificación _one-hot_ que conseguimos, por ejemplo, con la función `to_categorical` de `keras.utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b870abf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "print(y_train[:,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b3fad3",
   "metadata": {},
   "source": [
    "Como podemos ver, la salida de nuestro modelo es ahora una matriz de dimensión 4, donde cada fila es una clase y cada columna es un ejemplo. Cada fila tiene un 1 en la posición correspondiente a la clase que se predice, y un 0 en todas las demás posiciones.\n",
    "\n",
    "Ahora vamos a entrenar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebca32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=len(x_train), epochs=1000, validation_split=0.1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a44a441",
   "metadata": {},
   "source": [
    "Veamos su evaluación en un gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd1fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e03bb",
   "metadata": {},
   "source": [
    "Bueno, no es exactamente una clasificación, pero al menos parece ser mejor que una clasificación aleatoria. Realicemos una predicción para trabajar con ella."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1968f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_test = np.argmax(model.predict(x_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51135694",
   "metadata": {},
   "source": [
    "Cabe destacar que hemos aplicado la función `argmax` sobre las filas para obtener la clase predicha. Esto se debe a que la salida de la capa _softmax_ es un vector de probabilidades, y queremos obtener la clase que tiene la mayor probabilidad.\n",
    "\n",
    "Ahora hablemos un poco de las métricas de evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be6c75",
   "metadata": {},
   "source": [
    "#### Métricas de evaluación para problemas de clasificación multiclase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1d1bf",
   "metadata": {},
   "source": [
    "En realidad, las métricas utilizadas en este tipo de problemas son las mismas que hemos visto para la clasificación binaria. Precisión, sensibilidad, exactitud, etc., son medidas que evalúan el acierto frente al error.\n",
    "\n",
    "Sin embargo, merece la pena revisar la matriz de confusión para los casos en los que tenemos más de una clase. Vamos a ello."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e4e13",
   "metadata": {},
   "source": [
    "##### Comprender una matriz de confusión multiclase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b1c63f",
   "metadata": {},
   "source": [
    "Mientras que una matriz de confusión de $2$ por $2$ es intuitiva y fácil de entender, las matrices de confusión más grandes pueden llegar a ser bastantes confusas.\n",
    "\n",
    "Explorémoslas suponiendo que, en el ejemplo anterior, las cuatro clases corresponden a cuatro tipos distintos de pociones de vida resultantes de mezclar dos hierbas en distintas proporciones: excelente ($3$), grande ($2$), pequeña ($1$) y diminuta ($0$). La evaluación de cualquier clasificador sobre estos datos de diamante producirá una matriz de $4$ por $4$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_test, ŷ_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19cceb",
   "metadata": {},
   "source": [
    "Aunque la interpretación de la matriz de confusión se vuelve más y más complicada a medida que aumenta el número de clases, existe un método para simplificar conceptualmente el entendimieto de cualquier tipo de matriz, independientemente de su tamaño.\n",
    "\n",
    "Lo primero que hay que hacer es identificar qué clases son positivas y cuáles son negativas. Esto depende de la tarea a resolver, y no siempre es obvio. Si la clasificación es equilibrada, es decir, si cada clase es igual de importante, puede que no haya clases positivas ni negativas, pero este no suele ser el caso, y siempre suele haber clases que son más o menos interesantes para nuestra tarea.\n",
    "\n",
    "En nuestro ejemplo, como aventureros acérrimos que somos, nos interesa que la red neuronal implementada en nuestra varita de identificación de pociones clasifique mejor las pociones excelentes y grandes que las pequeñas y diminutas, principalmente porque el tamaño de la mochila es limitado y son las que nos mantendrán vivo más tiempo. En este caso, las etiquetas _ideal_ y _premium_ constituirán una clase positiva, y las demás etiquetas se considerarán negativas.\n",
    "\n",
    "Después de eso, podemos pasar a definir los términos de la matriz de confusión:\n",
    "\n",
    "1. **Verdaderos positivos**: Aquellos casos donde la poción real y la predicha son _excelente_ (tipo 1) o _grande_ (tipo 2),\n",
    "2. **Verdaderos negativos**: Aquellos casos donde cualquier etiqueta considerada negativa ha sido correctamente predicha,\n",
    "3. **Falsos positivos**: La poción es _pequeña_ o _minúscula_, pero ha sido predicha como _grande_ o _excelente_, y\n",
    "4. **Falsos negativos**: La poción es _grande_ o _excelente_, pero ha sido predicha como _pequeña_ o _diminuta_.\n",
    "\n",
    "Habiendo definido los 4 términos, encontrar cada uno de ellos en la matriz debería ser fácil. Y a partir de ahí, se pueden definir las medidas que hemos discutido previamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e82cf8c",
   "metadata": {},
   "source": [
    "##### Una nota sobre diferentes estrategias de clasificación multiclase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc655fad",
   "metadata": {},
   "source": [
    "Aquí hemos resuelto el problema de clasificación multiclase de una manera muy específica: estableciendo una correspondencia entre cada una de las clases con cada una de las neuronas de salida, denotando estas la probabilidad de que un ejemplo pertenezca o no a la clase que representan. Esta estrategia es un tipo dentro de lo que se conoce como _transformación binaria_ de la salida y vamos a hablar un poco sobre ello.\n",
    "\n",
    "Hay algoritmos que están específicamente diseñados para resolver problemas de clasificación binaria, como las redes neuronales. Cuando tenemos una única clase y queremos clasificar si un ejemplo pertenece o no a una clase determinada, basta con una neurona. Sin embargo, si tenemos más clases, existen principalmente Una neurona si tenemos dos clases y varias neuronas si tenemos más. La estrategia de transformación binaria se subdivide en dos, dependiendo del número de neuronas que usamos para la salida.\n",
    "\n",
    "###### Estrategia uno contra todos (OVR, del inglés _one-vs-rest_)\n",
    "\n",
    "También conociada como uno contra todos (OVA, del inglés _one-vs-all_). En este caso, tenemos **una neurona para cada clase**, y cada neurona indica la probabilidad de que un ejemplo pertenezca a esa clase. Por ejemplo, si en un problema de clasificación de imágenes tenemos las clases `Perro`, `Gato`, `Caballo` y `Vaca`, tendremos cuatro neuronas de salida:\n",
    "\n",
    "- Neurona 1: `Perro` vs. (`Gato`, `Caballo`, `Vaca`),\n",
    "- Neurona 2: `Gato` vs. (`Perro`, `Caballo`, `Vaca`),\n",
    "- Neurona 3: `Caballo` vs. (`Perro`, `Gato`, `Vaca`), y\n",
    "- Neurona 4: `Vaca` vs. (`Perro`, `Gato`, `Caballo`).\n",
    "\n",
    "Esta es la estrategia que hemos utilizado y se llama así porque una clase vence al resto en cada clasificación. Suele ser la más extendida.\n",
    "\n",
    "###### Estrategia uno contra uno (OVO, del inglés _one-vs-one_)\n",
    "\n",
    "En este caso, tenemos **una neurona para cada par de clases**. En este caso, tendremos $\\frac{N(N-1)}{2}$ neuronas de salida, y cada neurona indica la probabilidad de que un ejemplo pertenezca a una clase frente a otra. Con esta estrategia tendremos un total de 6 neuronas para el mismo ejemplo de antes.\n",
    "\n",
    "In this case, we have one neuron for each pair of classes, i.e. $\\frac{N(N-1)}{2}$, and each neuron indicates the probability that an example belongs to one class versus the other. With this strategy we get a total of 6 neurons for the same example.\n",
    "\n",
    "- Neurona 1: `Perro` vs. `Gato`\n",
    "- Neurona 2: `Perro` vs. `Caballo`\n",
    "- Neurona 3: `Perro` vs. `Vaca`\n",
    "- Neurona 4: `Gato` vs. `Caballo`\n",
    "- Neurona 5: `Gato` vs. `Vaca`\n",
    "- Neurona 6: `Caballo` vs. `Vaca`\n",
    "\n",
    "Esta estrategia se usa a menudo en [SVM](https://es.wikipedia.org/wiki/M%C3%A1quinas_de_vectores_de_soporte), otra técnica de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a07053",
   "metadata": {},
   "source": [
    "### Clasificación multietiqueta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e8ea81",
   "metadata": {},
   "source": [
    "En la clasificación multietiqueta, el objetivo es predecir múltiples etiquetas binarias o atributos para una instancia de entrada dada. A diferencia de la clasificación multiclase, donde una instancia se asigna a una sola clase, **la clasificación multietiqueta asigna una instancia a una o más clases**.\n",
    "\n",
    "Por ejemplo, en un problema de clasificación multietiqueta de reconocimiento de imágenes, una imagen puede tener múltiples objetos o atributos que pueden ser reconocidos, como `gato`, `perro`, `interior`, `exterior`, `diurno` y `nocturno`. Así, una imagen puede ser asignada múltiples etiquetas al mismo tiempo.\n",
    "\n",
    "En la clasificación multietiqueta, cada ejemplo puede estar asociado con múltiples etiquetas binarias, y el modelo tiene como objetivo aprender un mapeo entre las características de entrada y el conjunto correcto de etiquetas. Hay varios algoritmos que se pueden utilizar para la clasificación multietiqueta, como $k$-vecinos más cercanos, árboles de decisión, _random forest_ y redes neuronales.\n",
    "\n",
    "El problema a resolver será similar al anterior sólo que cada ejemplo podrá pertenecer a varias clases de las existentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = sklearn.datasets.make_multilabel_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=2,\n",
    "    n_classes=4,\n",
    "    n_labels=1,\n",
    ")\n",
    "Y_colors = Y[:,0] + 2*Y[:,1] + 4*Y[:,2] + 8*Y[:,3]\n",
    "plt.scatter(X[:,0], X[:,1], marker=\"+\", c=Y_colors, cmap='bwr');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377660a0",
   "metadata": {},
   "source": [
    "Al igual que antes, extraeremos un conjunto de entrenamiento y un conjunto de prueba para evaluar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = X[:-1000,:], Y[:-1000]\n",
    "y_train_colors = y_train[:,0] + 2*y_train[:,1] + 4*y_train[:,2] + 8*y_train[:,3]\n",
    "x_test, y_test = X[-1000:,:], Y[-1000:]\n",
    "y_test_colors = y_test[:,0] + 2*y_test[:,1] + 4*y_test[:,2] + 8*y_test[:,3]\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_train[:,0], x_train[:,1], marker=\"+\", c=y_train_colors, cmap='bwr');\n",
    "plt.title('Training set')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x_test[:,0], x_test[:,1], marker=\"+\", c=y_test_colors, cmap='bwr');\n",
    "plt.title('Test set');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeff87b6",
   "metadata": {},
   "source": [
    "#### Modelo de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b71c1d",
   "metadata": {},
   "source": [
    "Implementar un modelo de clasificación multietiqueta es muy similar a un problema de clasificación multiclase. Sin embargo en este caso, una capa _softmax_ en la salida y una entropía cruzada como _loss__ en la salida única no son válidas porque, por definición, puede haber más de una salida.\n",
    "\n",
    "En cambio, cada salida va a ser independiente, es decir, cada neurona de salida corresponderá a una clase y no habrá una capa posterior que las una. Para calcular la pérdida, utilizaremos una entropía cruzada (a saber, una entropía cruzada binaria) que se calculará para cada salida diferente a la vez. De esta manera, los errores de cada salida se calcularán por separado para cada una de las clases.\n",
    "\n",
    "Sabiendo esto, vamos a crear el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc1853",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=(2,)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='sigmoid'),\n",
    "])\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.SGD(),\n",
    "    metrics = [tf.keras.metrics.BinaryAccuracy()]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c06f9b0",
   "metadata": {},
   "source": [
    "Hemos hecho un poco de trampa porque este problema es un poco más complejo y requiere un modelo más potente.\n",
    "\n",
    "Estas cuatro salidas corresponderán a la codificación de nuestras cuatro clases, y cada una de ellas tendrá una salida independiente. En este caso, la salida será un número entre 0 y 1 que indica la probabilidad de que el ejemplo pertenezca a esa clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[:,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c8d71",
   "metadata": {},
   "source": [
    "La codificación es similar a la que hemos hecho antes (_one-hot encoding_), pero en este caso, cada fila puede tener más de un 1. En ocasiones es nombrada _multi-hot encoding_.\n",
    "\n",
    "Ahora sólo nos queda entrenar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c505158",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=len(x_train), epochs=1000, validation_split=0.1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f45e941",
   "metadata": {},
   "source": [
    "Vamos a ver cómo ha evolucionado el proceso de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81832dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3148bf26",
   "metadata": {},
   "source": [
    "Por lo visto parece que el modelo ha aprendido algo, aunque la evolución del _loss_ no es para emocionarse. Aún así, por ejemplo, es suficiente para nosotros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ_test = (model.predict(x_test) > 0.5).astype(int)\n",
    "print(ŷ_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e9c9d",
   "metadata": {},
   "source": [
    "#### Métricas de evaluación para problemas de clasificación multietiqueta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdb536a",
   "metadata": {},
   "source": [
    "A diferencia de los problemas de clasificación binaria y multiclase, en los que cada instancia está asociada a una única etiqueta de clase, en la clasificación multietiqueta, cada instancia puede estar asociada a múltiples etiquetas binarias. Por lo tanto, las métricas de evaluación utilizadas en la clasificación multietiqueta son diferentes de las utilizadas en los problemas de clasificación binaria y multiclase.\n",
    "\n",
    "Generalmente, esto se hace utilizando métricas como la precisión, la exactitud, el _recall_ y el F1 _score_, que pueden calcularse para cada etiqueta por separado o para el conjunto total de etiquetas. Sin embargo, hay otras métricas que pueden darnos algunas indicaciones de cómo los modelos están realizando su tarea.\n",
    "\n",
    "Estas métricas pueden calcularse por separado para cada etiqueta o para el conjunto total de etiquetas. Dependiendo del problema específico y del dominio, diferentes métricas pueden ser más apropiadas. Por lo tanto, es importante elegir cuidadosamente las métricas de clasificación multietiqueta apropiadas para evaluar el rendimiento de un modelo dado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eec65e",
   "metadata": {},
   "source": [
    "##### Macropromedio\n",
    "\n",
    "Consiste en calcular una métrica (por ejemplo, precisión, _recall_ o F1 _score_) para cada etiqueta por separado, y luego tomar la media de todas las métricas. Esta métrica da el mismo peso a cada etiqueta, independientemente de su frecuencia en el conjunto de datos.\n",
    "\n",
    "$$\n",
    "macro(f, y, \\hat{y}) = \\sum_{l \\in L} f(y, \\hat{y}, l)\n",
    "$$\n",
    "\n",
    "Siendo $L$ el conjunto de etiquetas o clases y $l$ la métrica. Por ejemplo, dado que queremos calcular el promedio macro para la precisión, tenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y, ŷ, label=1):\n",
    "    TP = np.sum((ŷ_test == label) & (y_test == label))\n",
    "    FP = np.sum((ŷ_test == label) & (y_test != label))\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "def macro(metric, y, ŷ, labels=None):\n",
    "    labels = labels or np.unique(y)\n",
    "    return np.mean([metric(y, ŷ, label) for label in labels])\n",
    "\n",
    "print(f'Macro precision = {macro(precision, y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcc3e7f",
   "metadata": {},
   "source": [
    "Especial atención a que hemos reescrito la función de precisión para que ahora tenga en cuenta la etiqueta. Dado que estamos tratando con un problema de más de una clase, es necesario indicar sobre qué etiqueta queremos trabajar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f14b4e",
   "metadata": {},
   "source": [
    "##### Micropromedio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0ceba",
   "metadata": {},
   "source": [
    "En esta medida se calcula la métrica considerando todos los ejemplos y etiquetas juntas. Esta métrica da el mismo peso a cada ejemplo y a cada etiqueta, independientemente de su frecuencia en el conjunto de datos.\n",
    "\n",
    "Por supuesto depende de la métrica a calcular, por lo que por ejemplo, para la precisión, tendríamos lo siguiente:\n",
    "\n",
    "$$\n",
    "\\mu Pre = \\frac{\\sum_{l in L}TP_l}{\\sum_{l in L}TP_l + \\sum_{l in L}FP_l}\n",
    "$$\n",
    "\n",
    "Lo cual se puede implementar de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c425e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_precision(y, ŷ, labels=None):\n",
    "    labels = labels or np.unique(y)\n",
    "    TP = np.sum([(ŷ_test == label) & (y_test == label) for label in labels])\n",
    "    FP = np.sum([(ŷ_test == label) & (y_test != label) for label in labels])\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "print(f'Micro precision = {micro_precision(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd003bf",
   "metadata": {},
   "source": [
    "##### Ratio de coincidencia exacta (EMR del inglés _exact match ratio_) y 1/0 _loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac0f721",
   "metadata": {},
   "source": [
    "Esta métrica es bastante simple y es una extensión del concepto de precisión. Se define como:\n",
    "\n",
    "$$\n",
    "EMR(y, \\hat{y}) = \\frac{1}{n} \\sum_{i = 0}^n I(y_i = \\hat{y_i})\n",
    "$$\n",
    "\n",
    "Donde $I$ es una función (denominada «indicador») que devuelve $1$ o $0$ si el argumento es `true` o `false` respectivamente, $n$ es el número de ejemplos en el conjunto de entrenamiento, $y$ es el vector de etiquetas verdaderas e $\\hat{y}$ es el vector de etiquetas predichas.\n",
    "\n",
    "En otras palabras, el ratio de coincidencia exacta es la proporción de ejemplos en los que todas las etiquetas verdaderas coinciden con todas las etiquetas predichas. Por ejemplo, si tenemos un conjunto de datos con $100$ ejemplos, y en $80$ de ellos todas las etiquetas verdaderas coinciden con todas las etiquetas predichas, el ratio de coincidencia exacta sería de $0,8$.\n",
    "\n",
    "Una posible implementación de esta métrica sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b38653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emr(y, ŷ):\n",
    "    return np.sum(y == ŷ) / len(y)\n",
    "\n",
    "print(f'EMR = {emr(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c375f",
   "metadata": {},
   "source": [
    "El ratio de coincidencia exacta es una métrica útil para evaluar el rendimiento de un modelo de clasificación multietiqueta, ya que proporciona una medida de cuántos ejemplos se clasifican correctamente en términos de todas las etiquetas.\n",
    "\n",
    "El principal inconveniente de esta medida es que no tiene en cuenta las etiquetas parcialmente correctas. Es decir, o es 100% correcta o no es correcta en absoluto. Por ejemplo, si estamos clasificando imágenes de perros y gatos, y el sistema predice que hay un perro y un gato en la imagen, pero en realidad hay un perro y un caballo, la medida EMR dará un 0.\n",
    "\n",
    "Por ello, a partir de esta métrica se define el _loss_ 1/0, que se define como:\n",
    "\n",
    "$$\n",
    "L_{01}(y, \\hat{y}) = 1 - EMR(y, \\hat{y})\n",
    "$$\n",
    "\n",
    "Dicho de otro modo, el _loss_ 1/0 es simplemente el complemento de la medida EMR. Es una medida útil para evaluar el rendimiento de un modelo de clasificación multietiqueta, ya que proporciona una medida de cuántos ejemplos se clasifican incorrectamente en términos de todas las etiquetas.\n",
    "\n",
    "Sin embargo, esta función de _loss_ tiene el problema de que, como no es diferenciable, no se puede utilizar en la mayoría de los algoritmos de aprendizaje automático. Pero la ponemos aquí para conocer de su existencia, no vaya a ser que nos la encontremos en algún sitio y no sepamos qué significa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d0f7bb",
   "metadata": {},
   "source": [
    "##### _Hamming loss_ (HL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d173d9b",
   "metadata": {},
   "source": [
    "Se define como la fracción de etiquetas que se predicen incorrectamente, es decir, la fracción de etiquetas incorrectas con respecto al número total de etiquetas:\n",
    "\n",
    "$$\n",
    "HL(y, ŷ) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{L} \\sum_{j=1}^{L} I(y_{ij} \\neq \\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "Una posible implementación de esta métrica sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be1bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_loss(y, ŷ):\n",
    "    return np.sum(y | ŷ) / np.prod(y.shape)\n",
    "\n",
    "print(f'Hamming loss = {hamming_loss(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e73a70",
   "metadata": {},
   "source": [
    "Se puede observar que se trata de una extensión de la métrica 1/0 _loss_, pero teniendo en cuenta esta vez las etiquetas parcialmente correctas. Esto es, si el sistema predice que hay un perro y un gato en la imagen, pero en realidad hay un perro y un caballo, la métrica HL dará un 0.5 y no un 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65235f2a",
   "metadata": {},
   "source": [
    "##### Coeficiente de similitud de Jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51de21",
   "metadata": {},
   "source": [
    "Se usa para cómo de parecido son dos conjuntos. En el contexto de la clasificación multietiqueta, se utiliza para medir cuán similares son las etiquetas verdaderas y las predichas. Se define como:\n",
    "\n",
    "$$\n",
    "J(y, \\hat{y}) = \\frac{|y \\cap \\hat{y}|}{|y \\cup \\hat{y}|}\n",
    "$$\n",
    "\n",
    "Dicho de otro modo, el coeficiente de similitud de Jaccard es la proporción de la intersección de las etiquetas verdaderas y las predichas con respecto a la unión de las etiquetas verdaderas y las predichas. Por ejemplo, si tenemos un conjunto de etiquetas verdaderas y predichas, y la intersección de las dos es de 10 y la unión de las dos es de 20, el coeficiente de similitud de Jaccard sería de 0,5.\n",
    "\n",
    "Una posible implementación de esta métrica sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a27ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_score(y_true, y_pred):\n",
    "    # Compute intersection and union of true and predicted labels\n",
    "    inter = np.sum(y_true & y_pred, axis=1)\n",
    "    union = np.sum(y_true | y_pred, axis=1)\n",
    "    \n",
    "    jaccard = np.divide(\n",
    "        inter,\n",
    "        union,\n",
    "        out=np.zeros_like(inter),\n",
    "        where=union!=0,\n",
    "        casting='unsafe'\n",
    "    )\n",
    "    \n",
    "    return np.mean(jaccard)\n",
    "\n",
    "print(f'Jaccard score = {jaccard_score(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f910ef8a",
   "metadata": {},
   "source": [
    "El coeficiente de similitud de Jaccard toma valores entre $0 y 1$, donde $1$ indica una similitud perfecta entre las etiquetas verdaderas y las predichas, y $0$ indica que no hay similitud entre las etiquetas verdaderas y las predichas. Un valor de $0,5$ indica, por tanto, que la mitad de las etiquetas verdaderas y predichas coinciden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05096b5",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65fcd4",
   "metadata": {},
   "source": [
    "Este _notebook_ ha sido intensito, pero en él hemos visto las principales diferencias de los dos tipos de problemas con los que nos encontraremos en problemas de aprendizaje profundo: clasificación y regresión. Los modelos desarrollados para estos son muy similares, variando básicamente en la salida y su cálculo de error.\n",
    "\n",
    "Además, para la evaluación de estos modelos hemos presentado algunas medidas, algunas específicas para la clasificación y otras para la regresión. Lo bueno es que prácticamente todos los _frameworks_ incluyen estas implementaciones, probablemente mucho mejor de lo que nosotros mismos podríamos implementarlas. Sin embargo, es muy importante saber cómo estamos midiendo y qué significan esas mediciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582efec2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
