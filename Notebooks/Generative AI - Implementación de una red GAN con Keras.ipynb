{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4480ff39",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Implementación de una red GAN con Keras<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto Díaz Álvarez<br>Última actualización: 2023-03-14</small></i></div>\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2489222",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17030ba",
   "metadata": {},
   "source": [
    "Las redes generativas adversariales (GAN, de _Generative Adversarial Networks_) fueron presentadas por primera vez por Goodfellow et al. en el artículo [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661) publicado en 2014. Este tipo de redes pueden ser utilizadas para la generación sintética de datos prácticamente idénticos a los originales.\n",
    "\n",
    "Para la generación de estos datos se usan dos redes neuronales durante el entrenamiento: la **generadora**, que acepta un vector de entrada de ruido generado aleatoriamente y produce los datos de salida de aspecto similar a los datos auténticos, y la **discriminadora**, que intenta determinar si los datos que se le presentan son auténticos o generados.\n",
    "\n",
    "Entrenando estas redes al mismo tiempo, una retroalimentando a la otra, dispondremos de un medio para generar datos prácticamente indistinguibles de los originales, o visto de otro modo, dispondremos de un medio para determinar si unos determinados datos son verdaderos o _fake_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d1e17",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9e324",
   "metadata": {},
   "source": [
    "Vamos a implementar una GAN simple que entrenaremos con el conjunto de datos `mnist`.\n",
    "\n",
    "Una vez consideremos que el modelo está correctamente entrenado, usaremos la red generadora del modelo para generar nuevos números los cuales serán (esperemos) completamente originales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d401eac",
   "metadata": {},
   "source": [
    "## Imports y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa528aec",
   "metadata": {},
   "source": [
    "A continuación importaremos las librerías que se usarán a lo largo del notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3a5f84",
   "metadata": {},
   "source": [
    "Asímismo, configuramos algunos parámetros para adecuar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d522d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (15, 8),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fec89",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7223928",
   "metadata": {},
   "source": [
    "## El dataset que usaremos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd6e8a4",
   "metadata": {},
   "source": [
    "Usaremos el _dataset_ `mnist`, que nos viene genial para un montón de tareas. Primero definiremos unas constantes que usaremos a lo largo del notebook para referirnos a datos del _dataset_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a9f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH, HEIGHT, CHANNELS = 28, 28, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f4b3a3",
   "metadata": {},
   "source": [
    "Como a lo que nos vamos a dedicar es a generar datos, la verdad es que nos dan un poco igual las labels. Nuestro objetivo aquí es que las dos redes aprendan a generar números manuscritos y a discernir si son o no originales, respectivamente.\n",
    "\n",
    "Por tanto, cargaremos los ejemplos de entrada para el conjunto de entrenamiento y de _test_ y los combinaremos en uno sólo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75709eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "dataset = np.concatenate((x_train, x_test))\n",
    "\n",
    "print(f'Real images: {dataset.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9977840",
   "metadata": {},
   "source": [
    "## Red generadora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d1f08d",
   "metadata": {},
   "source": [
    "La parte generadora de un modelo GAN se encarga de generar datos nuevos a partir de ruido aleatorio. Es de esperar que, a la larga, sea capaz de generar nuevos datos con una distribución similar a la de los elementos reales.\n",
    "\n",
    "Y ya que hemos dicho _ruido aleatorio_, vamos con la definición de la longitud del vector de valores de entrada de ruido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da234b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_FEATURES = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f710f56",
   "metadata": {},
   "source": [
    "A continuación definimos una función que nos devolverá una red generadora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(hidden_features, output_shape):\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.2), input_shape=(hidden_features,)),\n",
    "        tf.keras.layers.BatchNormalization(momentum=0.8),\n",
    "        tf.keras.layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.2)),\n",
    "        tf.keras.layers.BatchNormalization(momentum=0.8),\n",
    "        tf.keras.layers.Dense(WIDTH*HEIGHT*CHANNELS, activation='sigmoid'),\n",
    "        tf.keras.layers.Reshape(output_shape)\n",
    "    ], name='Generator')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851514b1",
   "metadata": {},
   "source": [
    "Esta red toma como entrada un vector de la longitud definida en la variable `HIDDEN_FEATURES`, y como salida dará una imagen del tamaño de la del conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1fb82",
   "metadata": {},
   "source": [
    "Una vez definida la función, creamos la red generadora que formará parte del modelo GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71f7e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = create_generator(HIDDEN_FEATURES, (WIDTH, HEIGHT, CHANNELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42328274",
   "metadata": {},
   "source": [
    "## Red discriminadora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834b969",
   "metadata": {},
   "source": [
    "Al igual que con la generadora, definiremos una función que crea una red discriminadora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b60201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator(input_shape,):\n",
    "    return tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(32, activation=tf.keras.layers.LeakyReLU(alpha=0.2)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "    ], name='Discriminator')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ab0337",
   "metadata": {},
   "source": [
    "Si observamos un poco la estructura, hemos añadido bastante regularización. El `mnist` es un problema muy simple, y la discriminadora es capaz de determinar muy rápidamente las características que hacen a una imagen real, por lo que aprende muy rápido a discrimiar, impidiendo que la red generadora aprenda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c834261b",
   "metadata": {},
   "source": [
    "Una vez definida la función, creamos la red discriminadora que formará parte de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f0dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = create_discriminator(input_shape=(WIDTH, HEIGHT, CHANNELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e200bbf",
   "metadata": {},
   "source": [
    "## Definición de la arquitectura GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b879f4",
   "metadata": {},
   "source": [
    "Ya disponemos de una red generadora y otra discriminadora. Ahora, antes de realizar la composición del modelo completo, compilaremos el discriminador para luego impedir el entrenamiento de sus pesos en el resto del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b432a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80162c60",
   "metadata": {},
   "source": [
    "A partir de este punto, cualquier modelo que use el discriminador, durante el paso de actualización de los parámetros la red discriminadora **no verá alterados sus pesos**. La única manera que existe ahora mismo de actualizar los pesos de la red discriminadora es invocando su grafo explícitamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfacfd33",
   "metadata": {},
   "source": [
    "El siguiente paso es crear la estructura donde se conectan las redes generadora y discriminadora y crear un nuevo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b09036",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.keras.layers.Input(shape=(HIDDEN_FEATURES,))\n",
    "validity = discriminator(generator(z))\n",
    "\n",
    "combined = tf.keras.models.Model(z, validity)\n",
    "combined.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1cf685",
   "metadata": {},
   "source": [
    "El modelo `combined` es nuestro modelo GAN, y se invocará cuando queramos entrenar a la red generadora del modelo. \n",
    "\n",
    "Cuando vayamos a entrenar a la red generadora, lo haremos a través de este modelo, dando como entrada un vector de ruido y como salida siempre 1 (es decir, diciéndole al modelo que la imagen es real). Durante la actualización de pesos **no se tocará el discriminador**, sino que sólo se verán afectados los pesos de la red generadora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b53e26e",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670e813e",
   "metadata": {},
   "source": [
    "Ahora sí, podemos proceder al entrenamiento. A modo de resume, el proceso es el siguiente:\n",
    "\n",
    "1. Creamos un `ImageDataGenerator` sobre el conjunto de datos de imágenes que nos estará dando constantemente _batchs_ de un tamaño determinado sin parar, comenzando desde el principio si llega al final.\n",
    "1. Luego, en un bucle infinito:\n",
    "    1. Tomará un batch de imágenes reales $\\mathcal{R}$.\n",
    "    2. Generará un batch de vectores de ruido aleatorio del mismo tamaño y, a partir de él, un batch de imágenes ficticias $\\mathcal{F}$.\n",
    "    3. Entrenará al discriminador indicando que $\\mathcal{R}$ son verdaderas y $\\mathcal{F}$ son falsas.\n",
    "    4. Entrenará al generador indicando que $\\mathcal{F}$ son verdaderas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a14bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, generator, discriminator, combined, sample_interval=100, window=None):\n",
    "    window = window or sample_interval * 5\n",
    "    # Para la salida de la gráfica por pantalla\n",
    "    fig,ax = plt.subplots(1,1)\n",
    "    ax.set_xlabel('Task')\n",
    "    ax.set_ylabel('Reward')\n",
    "    \n",
    "    # Añadimos una dimensión porque lo requiere el datagenerator\n",
    "    dataset = dataset.reshape(*dataset.shape, CHANNELS)\n",
    "    # Creamos un datagenerator para que nos vaya dando pequeños batches de\n",
    "    # entrenamiento\n",
    "    data_generator = tf.keras.preprocessing.image.ImageDataGenerator().flow(\n",
    "        dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    history = {\n",
    "        'epoch': collections.deque(maxlen=window),\n",
    "        'd_loss' : collections.deque(maxlen=window),\n",
    "        'd_acc' : collections.deque(maxlen=window),\n",
    "        'g_loss' : collections.deque(maxlen=window),\n",
    "    }\n",
    "\n",
    "    epoch = 0\n",
    "    for epoch, real_batch in enumerate(data_generator):\n",
    "        # Creamos las etiquetas de imagenes válidas y no válidas para el batch\n",
    "        fake_labels = np.zeros((real_batch.shape[0], 1))\n",
    "        real_labels = np.ones((real_batch.shape[0], 1))\n",
    "\n",
    "        # Creamos un batch del mismo tamaño que el original, pero con imágenes\n",
    "        # falsas. Cada una se creará a partir de un vector de características\n",
    "        # diferente\n",
    "        noise = np.random.random((real_batch.shape[0], HIDDEN_FEATURES))\n",
    "        fake_batch = generator.predict(noise, verbose=0)\n",
    "\n",
    "        # Entrenamos el generador sólo con las imágenes falsas (diciendo que\n",
    "        # son verdaderas)\n",
    "        generator_loss = combined.train_on_batch(noise, real_labels)\n",
    "\n",
    "        # Entrenamos el discriminador con las imágenes verdaderas y falsas\n",
    "        discriminator_loss_real = discriminator.train_on_batch(real_batch, real_labels)\n",
    "        discriminator_loss_fake = discriminator.train_on_batch(fake_batch, fake_labels)\n",
    "        discriminator_loss = .5 * np.add(discriminator_loss_real, discriminator_loss_fake)\n",
    "\n",
    "        history['epoch'].append(epoch)\n",
    "        history['d_loss'].append(discriminator_loss[0])\n",
    "        history['d_acc'].append(discriminator_loss[1])\n",
    "        history['g_loss'].append(generator_loss)\n",
    "\n",
    "        # Pintamos las gráficas\n",
    "        if epoch % sample_interval == 0:\n",
    "            ax.cla()\n",
    "            ax.set_title(f'Epoch {epoch} (D loss: {discriminator_loss[0]:.4}, D acc: {discriminator_loss[1]:.4}, G loss: {generator_loss:.4})')\n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.set_ylabel('Loss')\n",
    "\n",
    "            ax.plot(history['epoch'], history['d_loss'], label='Discriminator loss')\n",
    "            ax.plot(history['epoch'], history['g_loss'], label='Generator loss')\n",
    "            ax.legend()\n",
    "            fig.canvas.draw()\n",
    "            #generator.save('tmp/generator.h5')\n",
    "            #discriminator.save('tmp/discriminator.h5')\n",
    "        epoch += 1\n",
    "\n",
    "\n",
    "train(dataset, generator, discriminator, combined, sample_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ccde2",
   "metadata": {},
   "source": [
    "Tras parar (manualmente) el entrenamiento, podemos echar un ojo a algunas de las imágenes generadas por la red generadora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ddf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "noise = np.random.normal(0, 1, (5, HIDDEN_FEATURES))\n",
    "images = generator(noise)\n",
    "\n",
    "fig, ax = plt.subplots(1,5, figsize=(15, 5))\n",
    "for i, image in enumerate(images):\n",
    "    ax[i].imshow(image)\n",
    "    ax[i].axis('off')\n",
    "plt.savefig('aa.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43585490",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7defd88",
   "metadata": {},
   "source": [
    "Hemos aprendido a implementar una red GAN desde cero (bueno, usando Keras, pero se me entiende) y hemos podido explorar algunos de sus desafíos. En particular, hemos observado lo complicado que es conseguir que ambas redes, el generador y el discriminador, converjan correctamente.\n",
    "\n",
    "Ahora ya somos capaces de empezar a ver los desafíos que existen en estos tipos de redes (como por ejemplo el maldito colapso modal después de haber sufrido la discriminadora perfecta), cómo se ajustan los hiperparámetros y, en definitiva, cuál es la base en la que se apoyan las técnicas de redes generativas adversariales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33005bde",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
