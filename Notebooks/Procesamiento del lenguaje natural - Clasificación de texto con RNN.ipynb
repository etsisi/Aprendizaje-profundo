{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4480ff39",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Clasificación de texto con RNN<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Última actualización: 2023-05-11</small></i></div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2489222",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17030ba",
   "metadata": {},
   "source": [
    "En un _notebook_ anterior exploramos la clasificación de textos con CNN, las cuales son adecuadas para capturar dependencias locales en datos de texto. Sin embargo, a veces necesitamos un modelo más potente que pueda capturar dependencias a largo plazo en los datos. Aquí es donde entran en juego las RNN.\n",
    "\n",
    "Las RNN están diseñadas específicamente para modelar datos secuenciales, lo que las hace ideales para tareas de clasificación de texto. A diferencia de las redes neuronales tradicionales, que procesan las entradas independientemente unas de otras, las RNN mantienen una memoria de las entradas anteriores y utilizan esta información para hacer predicciones sobre la entrada actual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d1e17",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9e324",
   "metadata": {},
   "source": [
    "Vamos a explorar cómo utilizar RNN para la clasificación de texto en el mismo problema que en el _notebook_ donde clasificábamos las reseñas de Amazon que los usuarios hicieron sobre los productos mediante CNN, pero esta vez con RNN.\n",
    "\n",
    "Veremos que en realidad los cambios son mínimos, ya que es poco más que cambiar una capa por otra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d401eac",
   "metadata": {},
   "source": [
    "## Bibliotecas y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa528aec",
   "metadata": {},
   "source": [
    "A continuación importaremos las bibliotecas que se utilizarán a lo largo del cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import requests\n",
    "from shutil import unpack_archive\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3a5f84",
   "metadata": {},
   "source": [
    "También configuraremos algunos parámetros para adaptar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d522d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76e288",
   "metadata": {},
   "source": [
    "Y crearemos los directorios necesarios en caso de que no se hayan creado previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c7f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('tmp', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fec89",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7223928",
   "metadata": {},
   "source": [
    "## Parámetros comunes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9977840",
   "metadata": {},
   "source": [
    "Mantendremos los mismos parámetros globales para poder comparar ambos métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuántas dimensiones tienen nuestros vectores de palabras (usaremos\n",
    "# GloVe, así que 50, 100, 200 o 300)\n",
    "EMBEDDING_DIM = 300\n",
    "# Tamaño máximo de nuestro vocabulario (se elegirán los token más\n",
    "# frecuentes)\n",
    "MAX_VOCAB_SIZE = 16384\n",
    "# Longitud máxima de las secuencias de palabras\n",
    "MAX_SEQUENCE_LEN = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd945b",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aa0426",
   "metadata": {},
   "source": [
    "El proceso que llevaremos a cabo será el mismo que hicimos en el _notebook_ anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e5df57",
   "metadata": {},
   "source": [
    "### Descarga del _dataset_ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a1ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = 'https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Digital_Music_5.json.gz'\n",
    "DATASET_ZIP = 'tmp/Digital_Music_5.json.gz'\n",
    "\n",
    "# Download the remote file if it does not exist\n",
    "if not os.path.exists(DATASET_ZIP):\n",
    "    with open(DATASET_ZIP, 'wb') as f:\n",
    "        print(f'Downloading {DATASET_ZIP}...')\n",
    "        r = requests.get(DATASET_URL, verify=False)\n",
    "        f.write(r.content)\n",
    "        print('OK')\n",
    "\n",
    "corpus = pd.read_json(DATASET_ZIP, lines=True)\n",
    "corpus.dropna(subset=['overall', 'reviewText'], inplace=True)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdcdc82",
   "metadata": {},
   "source": [
    "### ... preparación de las entradas ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b853ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = corpus['reviewText'].astype(str).str.strip()\n",
    "print(f'Training input shape: {x_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933c217",
   "metadata": {},
   "source": [
    "### ... de las salidas ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f8283",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = corpus['overall'].astype(int).replace({\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 1,\n",
    "    4: 2,\n",
    "    5: 2,\n",
    "})\n",
    "print(f'Training output shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413fefc",
   "metadata": {},
   "source": [
    "### ... la capa de vectorización de texto (_TextVectorization_) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3221f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_VOCAB_SIZE + 2,\n",
    "    output_sequence_length=MAX_SEQUENCE_LEN,\n",
    "    name='vectorization',\n",
    ")\n",
    "vectorize_layer.adapt(x_train)\n",
    "\n",
    "print(f'Vocabulary length: {len(vectorize_layer.get_vocabulary())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dac35d",
   "metadata": {},
   "source": [
    "### ... los _embeddings_ preentrenados de GloVe ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "GLOVE_FILE = 'tmp/glove.6B.zip'\n",
    "\n",
    "# Download the compressed GloVe dataset (if you don't already have it)\n",
    "if not os.path.exists(GLOVE_FILE):\n",
    "    print('Downloading ...', end='')\n",
    "    with open(GLOVE_FILE, 'wb') as f:\n",
    "        r = requests.get(GLOVE_URL, allow_redirects=True)\n",
    "        f.write(r.content)\n",
    "    print('OK')\n",
    "\n",
    "# Unzip it in the directory 'glove'.\n",
    "print('Unpacking ...', end='')\n",
    "unpack_archive(GLOVE_FILE, 'tmp')\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ece6e",
   "metadata": {},
   "source": [
    "### ... cogiendo los pesos de las `MAX_VOCAB_SIZE` palabras más comunes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b548d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Loading GloVe {EMBEDDING_DIM}-d embedding... ', end='')\n",
    "word2vec = {}\n",
    "with open(f'tmp/glove.6B.{EMBEDDING_DIM}d.txt') as f:\n",
    "    for line in f:\n",
    "        word, vector = line.split(maxsplit=1)\n",
    "        word2vec[word] = np.fromstring(vector,'f', sep=' ')\n",
    "print(f'done ({len(word2vec)} word vectors loaded)')\n",
    "\n",
    "print('Creating embedding matrix with GloVe vectors... ', end='')\n",
    "# Our newly created embedding: a matrix of zeros\n",
    "embedding_matrix = np.zeros((MAX_VOCAB_SIZE + 2, EMBEDDING_DIM))\n",
    "\n",
    "ko_words = 0\n",
    "for i, word in enumerate(vectorize_layer.get_vocabulary()):\n",
    "    if word == '[UNK]':\n",
    "        # The second word is for an unknown token, in glove is 'unk'\n",
    "        word = 'unk'\n",
    "\n",
    "    # Get the word vector and overwrite the row in its corresponding position\n",
    "    word_vector = word2vec.get(word)\n",
    "    if word_vector is not None:\n",
    "        embedding_matrix[i] = word_vector\n",
    "    else:\n",
    "        ko_words += 1\n",
    "print(f'done ({ko_words} words unassigned)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbce887",
   "metadata": {},
   "source": [
    "### ... y cargándolos dentro de la capa de `Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953aff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=embedding_matrix.shape[0],\n",
    "    output_dim=embedding_matrix.shape[1],\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LEN,\n",
    "    trainable=False,\n",
    "    name='Embedding',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6eedd5",
   "metadata": {},
   "source": [
    "## Clasificación basada en redes neuronales recurrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0758b",
   "metadata": {},
   "source": [
    "Y ahora, en lugar de utilizar CNN, utilizaremos RNN. En este caso, el conjunto de dimensiones lo realizan la capa `TextVectorization` (que convierte el texto en secuencias de enteros de longitud $T$) y la capa Embedding (que convierte cada entero en un vector de dimensiones $D$), convirtiendo la entrada en un tensor con la forma $N \\times T \\times D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    vectorize_layer,\n",
    "    embedding_layer,\n",
    "    tf.keras.layers.GRU(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['sparse_categorical_accuracy'],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ee23e3",
   "metadata": {},
   "source": [
    "Aunque el número de parámetros es similar, aumentar el número de unidades en una unidad recurrente no aumenta mucho el número de parámetros en nuestro modelo. Sin embargo, sí que aumentará mucho el tiempo de entrenamiento. Por lo tanto, nuestro modelo no podrá obtener resultados tan buenos como los anteriores.\n",
    "\n",
    "Entrenemos el modelo y esperemos que todo vaya bien (otra vez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd1f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade383c",
   "metadata": {},
   "source": [
    "Echemos un vistazo al progreso del entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d202e83a",
   "metadata": {},
   "source": [
    "Veamos ahora cómo interpreta el sentimiento de una reseña buena, regular y mala extraída del sitio web de amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "good = \"My nephew is on the autism spectrum and likes to fidget with things so I knew this toy would be a hit. Was concerned that it may not be \\\"complex\\\" enough for his very advanced brain but he really took to it. Both him (14 yrs) and his little brother (8 yrs) both enjoyed playing with it throughout Christmas morning. I'm always happy when I can find them something unique and engaging.\"\n",
    "poor = \"I wasn't sure about this as it's really small. I bought it for my 9 year old grandson. I was ready to send it back but my daughter decided it was a good gift so I'm hoping he likes it. Seems expensive for the price though to me.\"\n",
    "evil = \"I just wanted to follow up to say that I reported this directly to the company and had no response. I have not gotten any response from my review. The level of customer service goes a long way when an item you purchase is defective and this company didn’t care to respond. No I am even more Leary about ordering anything from this company. I never asked for a refund or replacement since I am not able to return it. I’m just wanted to let them know that this was a high dollar item and I expected it to be a quality item. Very disappointed! I bought this for my grandson for Christmas. He loved it and played with it a lot. My daughter called to say that the stickers were peeling on the corners. I am not able to take it from my grandson because he is autistic and wouldn’t understand. I just wanted to warn others who are wanting to get this. Please know that this is a cool toy and it may not happen to yours so it is up to you.\"\n",
    "\n",
    "probabilities = model.predict([good, poor, evil], verbose=0)\n",
    "print(f'Good was classified as {np.argmax(probabilities[0])}')\n",
    "print(f'Poor was classified as {np.argmax(probabilities[1])}')\n",
    "print(f'Evil was classified as {np.argmax(probabilities[2])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43585490",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7defd88",
   "metadata": {},
   "source": [
    "Hemos estudiado cómo utilizar RNN para clasificar textos y las hemos comparado con las CNN. Aunque ambas arquitecturas pueden ser eficaces para la clasificación de textos, presentan algunas diferencias clave.\n",
    "\n",
    "La principal es las RNN son más adecuadas para captar las dependencias a largo plazo en los datos de texto, mientras que las CNN son más adecuadas para captar las dependencias locales. Esto hace que las RNN sean una buena opción para tareas como el análisis de sentimientos o la traducción de idiomas, donde el contexto de una palabra o frase puede ser crucial para determinar su significado.\n",
    "\n",
    "Sin embargo, el entrenamiento de las RNN puede ser más costoso que el de las CNN (ya hemos visto el tiempo que tarda una red pequeña en ser entrenada durante 5 _epochs_). Por otro lado, las CNN son más rápidas de entrenar y se adaptan mejor a conjuntos de datos más grandes. Son especialmente adecuadas para tareas como la clasificación de textos o el reconocimiento de imágenes, en las que las características locales son importantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33005bde",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
