{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4480ff39",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Generating images with Variational Autoencoders<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto Díaz Álvarez<br>Última actualización: 2023-05-01</small></i></div>\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2489222",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17030ba",
   "metadata": {},
   "source": [
    "Variational Autoencoders (VAEs) are a class of generative models that have become very popular in recent years, thanks to their ability to generate high-quality images and other types of data.\n",
    "\n",
    "| <img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2021/07/01/ML1533-image003.jpg\" alt=\"Variational Autoencoder\" width=\"50%\"> | \n",
    "|:--:| \n",
    "| *Architecture diagram of a variational autoencoder (VAE). Source: [Deploy variational autoencoders for anomaly detection with TensorFlow Serving on Amazon SageMaker](https://aws.amazon.com/es/blogs/machine-learning/deploying-variational-autoencoders-for-anomaly-detection-with-tensorflow-serving-on-amazon-sagemaker/) (last visited May 01, 2023).* |\n",
    "\n",
    "Unlike traditional autoencoders, which are mainly used for dimensionality reduction and data compression, VAEs allow for the generation of new instances of data from the encoding of input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d1e17",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9e324",
   "metadata": {},
   "source": [
    "We will be implementing a VAE in Keras for the specific example of generating handwritten digit images using the mnist dataset. Our goal is to train a VAE on this dataset so that we can use the model to generate new digit images that resemble the original images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d401eac",
   "metadata": {},
   "source": [
    "## Libraries and configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa528aec",
   "metadata": {},
   "source": [
    "Next we will import the libraries that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e97df",
   "metadata": {},
   "source": [
    "We will also configure some parameters to adapt the graphical presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca72f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fec89",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7223928",
   "metadata": {},
   "source": [
    "## Conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d3a3bc",
   "metadata": {},
   "source": [
    "Of course, we will continue to work with the `fashion_mnist` dataset. By now we know it quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07075092",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255, x_test / 255\n",
    "\n",
    "print(f'Training shape: {x_train.shape} input')\n",
    "print(f'Test shape:     {x_test.shape} input')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0bf2ac",
   "metadata": {},
   "source": [
    "##  Implementation of the variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9a4c1e",
   "metadata": {},
   "source": [
    "We are going to perform a similar implementation to the vanilla autoencoder. However, while in a basic autoencoder the latent space is simply a compressed representation of the input data, in a VAE, this space is also used to generate new data samples, i.e., data that are not found in the original data set.\n",
    "\n",
    "This is achieved through the introduction of additional layers to calculate the mean and variance of the latent distribution. For this we will also need a KL divergence term (a measure of similarity) to ensure that the latent distribution approximates a standard normal distribution, using this similarity as part of the loss to try to get it to 0 (maximum similarity between distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191e820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(tf.keras.models.Model):\n",
    "\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        flatten_dim = None\n",
    "        if isinstance(input_dim, (list, tuple)):\n",
    "            flatten_dim = math.prod(input_dim)\n",
    "        elif isinstance(input_dim, int):\n",
    "            flatten_dim = input_dim\n",
    "            input_dim = (input_dim,)\n",
    "        else:\n",
    "            raise ValueError('Argument input_dim must be a tuple or an int')\n",
    "\n",
    "        # Encoder\n",
    "        encoder_input = tf.keras.layers.Input(shape=input_dim)\n",
    "        encoder_flatten = tf.keras.layers.Flatten()(encoder_input)\n",
    "        \n",
    "        z_mean = tf.keras.layers.Dense(latent_dim)(encoder_flatten)\n",
    "        z_log_sigma = tf.keras.layers.Dense(latent_dim)(encoder_flatten)\n",
    "        \n",
    "        def sampling(args):\n",
    "            z_mean, z_log_sigma = args\n",
    "            epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim), mean=0., stddev=0.1)\n",
    "            return z_mean + tf.math.exp(z_log_sigma) * epsilon\n",
    "\n",
    "        z = tf.keras.layers.Lambda(sampling)([z_mean, z_log_sigma])\n",
    "        \n",
    "        self.encoder = tf.keras.models.Model(encoder_input, [z_mean, z_log_sigma, z], name='encoder')\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(flatten_dim, activation='sigmoid'),\n",
    "            tf.keras.layers.Reshape(input_dim)\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_sigma, z = self.encoder(inputs)\n",
    "        reconstructed_input = self.decoder(z)\n",
    "        \n",
    "        # We add the KL divergence as loss\n",
    "        kl_loss = -tf.reduce_mean(z_log_sigma - tf.square(z_mean) - tf.exp(z_log_sigma) + 1) / 2\n",
    "        self.add_loss(kl_loss)\n",
    "\n",
    "        return reconstructed_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae11f96",
   "metadata": {},
   "source": [
    "Now we will create our VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e6395",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 256\n",
    "IMG_SIZE = (28, 28)\n",
    "\n",
    "vae = VariationalAutoencoder(IMG_SIZE, LATENT_DIM)\n",
    "vae.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f50862",
   "metadata": {},
   "source": [
    "And we train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb7a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = vae.fit(x_train, x_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e01cb",
   "metadata": {},
   "source": [
    "Let's see how the loss progresses during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5512f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2f9a78",
   "metadata": {},
   "source": [
    "This architecture is a bit more complex, which results in longer training. Let's see how it reconstructs our training images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c398e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "images = np.array(random.sample(list(x_train), n))\n",
    "\n",
    "encoded, _, _ = vae.encoder(images)\n",
    "decoded = vae.decoder(encoded).numpy()\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title('Original')\n",
    "\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded[i])\n",
    "    plt.title('Reconstructed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d4bea0",
   "metadata": {},
   "source": [
    "Now let's move on to the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eacbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(random.sample(list(x_test), n))\n",
    "\n",
    "encoded, _, _ = vae.encoder(images)\n",
    "decoded = vae.decoder(encoded).numpy()\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title('Original')\n",
    "\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded[i])\n",
    "    plt.title('Reconstructed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93597c3",
   "metadata": {},
   "source": [
    "This kind of network has the advantage that the coding is performed in a latent space in which the interpolations between elements are continuous, in the sense that they share characteristics between the elements they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a44da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 images (10 x 10 matrix) of 28 x 28\n",
    "num_elements = 10\n",
    "size=28\n",
    "figure = np.zeros((IMG_SIZE[0] * num_elements, IMG_SIZE[1] * num_elements))\n",
    "\n",
    "# We walk through the latent space between the boundaries of these values\n",
    "images = np.array(random.sample(list(x_test), 2))\n",
    "_, _, encoded_test = vae.encoder(images)\n",
    "min_z, max_z = np.min(encoded_test), np.max(encoded_test)\n",
    "grid_x = np.linspace(min_z, max_z, num_elements)\n",
    "grid_y = np.linspace(min_z, max_z, num_elements)\n",
    "# We plot the images that correspond to that space\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = [(xi if i % 2 == 1 else yi) for i in range(LATENT_DIM)]\n",
    "        x_decoded = vae.decoder.predict([z_sample], verbose=0)\n",
    "        digit = x_decoded[0].reshape(size, size)\n",
    "        figure[i * size: (i + 1) * size, j * size: (j + 1) * size] = digit\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(figure)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d440a7",
   "metadata": {},
   "source": [
    "However, these types of networks are not suitable for noise. For example, let's see what happens when we add a minimum of noise to our images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed65517",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(random.sample(list(x_test), n))\n",
    "noise_factor = 0.1\n",
    "noisy_images = images + noise_factor * tf.random.normal(shape=images.shape)\n",
    "noisy_images = tf.clip_by_value(noisy_images, clip_value_min=0, clip_value_max=1)\n",
    "\n",
    "_, _, encoded = vae.encoder(noisy_images)\n",
    "decoded = vae.decoder(encoded).numpy()\n",
    "\n",
    "plt.figure(figsize=(12,12)) \n",
    "for i in range(n):\n",
    "    ax = plt.subplot(3, n, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title('Original')\n",
    "\n",
    "    ax = plt.subplot(3, n, i + 1 + n)\n",
    "    plt.imshow(noisy_images[i])\n",
    "    plt.title('Noisy')\n",
    "\n",
    "    ax = plt.subplot(3, n, i + 1 + 2 * n)\n",
    "    plt.imshow(decoded[i])\n",
    "    plt.title('Reconstructed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43585490",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7defd88",
   "metadata": {},
   "source": [
    "We have implemented a VAE in Keras that has been able to encode and decode handwritten digits. As was the case with vanilla autoencoders, the encoding and subsequent decoding has been almost lossless, with the added advantage that encodings close to several \"sample sets\" interpolate them, acquiring characteristics of these sets.\n",
    "\n",
    "However, we have seen that they can be deficient in some cases, for example in denoising.\n",
    "\n",
    "This implementation can serve as a basis for the exploration of other more complex architectures in other problems, so do not hesitate to use them and explore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33005bde",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
