{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010176d1",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "# Underfitting and Overfitting<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Authors: Alberto Díaz Álvarez<br>Last update: 2023-04-09</small></i></div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554fb61b",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce42fc3a",
   "metadata": {},
   "source": [
    "Underfitting and overfitting are two critical elements to master in machine learning.\n",
    "\n",
    "In deep learning, **more**, especially the second one (overfitting), since the models are in general much more complex than those of traditional Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f243ba3",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d6959",
   "metadata": {},
   "source": [
    "We will explore both concepts with two simple but illustrative examples of the problems of underfitting (also bias) and overfitting (also _variance_) in regression and classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67582fcc",
   "metadata": {},
   "source": [
    "## Libraries and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb70d6",
   "metadata": {},
   "source": [
    "Next we will import the libraries that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58374f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e97df",
   "metadata": {},
   "source": [
    "We will also configure some parameters to adapt the graphic presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca72f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06541e93",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e0dd2",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Underfitting_ occurs when the model is not complex enough to capture the relationships between input and output variables, resulting in low accuracy in predicting output values in both training and test data.\n",
    "\n",
    "On the other hand, overfitting occurs when the model is too complex and fits the training data too closely, resulting in high accuracy on the training data but low accuracy on the test data. This is because the model is capturing noise and wrong relationships present in the training data rather than the true underlying relationships.\n",
    "\n",
    "To avoid these problems, it is necessary to find a balance between the complexity of the model and its ability to generalize to new and unseen data.\n",
    "\n",
    "The problem we will be working on is that of trying to approximate random values extracted from a sine function (although the idea is that in the problems we do not know where these values come from and that is the point of using models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1534cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples = 32\n",
    "noise_factor = 0.1\n",
    "\n",
    "f = lambda x: np.sin(2 * np.pi * x)\n",
    "true_x = np.linspace(0, 1, 100)\n",
    "x = np.sort(np.random.rand(n_samples))\n",
    "y = f(x) + np.random.randn(n_samples) * noise_factor\n",
    "\n",
    "plt.scatter(x, y, label='Sample')\n",
    "plt.plot(true_x, f(true_x), label='Original', alpha=0.1)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34aa905",
   "metadata": {},
   "source": [
    "### Underfitted model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a26c50",
   "metadata": {},
   "source": [
    "We will start with a model so small that it is not able to capture all the detail of the examples and therefore not able to fit the values well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e2cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(2, activation='relu', input_dim=1),\n",
    "    tf.keras.layers.Dense(1, activation='tanh')\n",
    "])\n",
    "model.compile(loss='mae', optimizer='adam', metrics = [tf.keras.metrics.RootMeanSquaredError()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x, y, epochs=1000, batch_size=len(x), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093b405b",
   "metadata": {},
   "source": [
    "Let's see how well the training turned out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a60ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08169a8",
   "metadata": {},
   "source": [
    "It can be seen that the error decreases, but it seems to stagnate in values that can be considered bad (the data belong to the $[0, 1]$ interval).\n",
    "\n",
    "Veamos qué forma tiene la función con la que nuestro modelo describe a los puntos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f5577",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, label='Sample')\n",
    "plt.plot(true_x, f(true_x), label='Original', alpha=0.2)\n",
    "plt.plot(true_x, model.predict(true_x), label='Predicted')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067bf0e0",
   "metadata": {},
   "source": [
    "Indeed, it has tried, but one can not get something out of nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d99fd12",
   "metadata": {},
   "source": [
    "### Overfitted model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6ebe41",
   "metadata": {},
   "source": [
    "Now we will go to the opposite extreme. What happens when our model is extremely large? Well, it learns the training set so much that it is not able to generalize, and when new examples come to it it fails to classify them. Let's train a large model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eef8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(1024, activation='relu', input_dim=1),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='tanh')\n",
    "])\n",
    "model.compile(loss='mae', optimizer='adam', metrics = [tf.keras.metrics.RootMeanSquaredError()])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x, y, epochs=2500, batch_size=len(x), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df965749",
   "metadata": {},
   "source": [
    "Let's see how the training went:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c15a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4b0703",
   "metadata": {},
   "source": [
    "Well, it seems to have learned quite a bit, but the validation error has skyrocketed. This is showing us that yes, it has learned the training set very well, but when there are new values the model has no idea where to fit them.\n",
    "\n",
    "Let's see what shape the function modeled by our network has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfbe323",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, label='Sample')\n",
    "plt.plot(true_x, f(true_x), label='Original', alpha=0.2)\n",
    "plt.plot(true_x, model.predict(true_x), label='Predicted')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f27fe1",
   "metadata": {},
   "source": [
    "Well, you have learned what you could, but it is too large a model that is not able to generalize sufficiently.\n",
    "\n",
    "A good exercise to do now would be to try to obtain a model that approximates this function as closely as possible from its example values. The truth is that, although fitting a model of which we know the origin function is very implausible, the exercise is more for playing with Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45b0a9f",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd3329",
   "metadata": {},
   "source": [
    "In classification problems, _underfitting_ and _overfitting_ are problems similar to those encountered in regression: either they fall short in capturing the relationships between input and output, or they overspecialize and are unable to generalize.\n",
    "\n",
    "To avoid them the remedy is the same: try to select a model with adequate complexity by studying the _loss_ trends in training and testing to evaluate its generalization capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55f27ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array(['red', 'blue'])\n",
    "n_samples = 50\n",
    "\n",
    "X, y = datasets.make_moons(n_samples=n_samples, noise=.5)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], label='Sample', color=colors[y])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca09487",
   "metadata": {},
   "source": [
    "### Underfitted model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69987435",
   "metadata": {},
   "source": [
    "Let's create a simple model to try to classify these two example sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca3122",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(2, activation='relu', input_dim=2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['binary_accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X, y, epochs=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b122b9d",
   "metadata": {},
   "source": [
    "After training, let's look at the trend of the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac03fcd",
   "metadata": {},
   "source": [
    "Wow, a similar case to the previous one. The training is stuck at a relatively high error. Let's see which regions the classifier has determined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dd437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridx1, gridx2 = np.meshgrid(np.linspace(-2.5, 2.5, 500), np.linspace(-2.5, 2.5, 500))\n",
    "grid = np.c_[gridx1.flatten(), gridx2.flatten()]\n",
    "probs = model.predict(grid, verbose=0)\n",
    "\n",
    "plt.xlim(-2.5, 2.5)\n",
    "plt.ylim(-2.5, 2.5)\n",
    "plt.pcolor(gridx1, gridx2, probs.reshape(500, 500))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:,0], X[:,1], color=colors[y]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732c465d",
   "metadata": {},
   "source": [
    "Nearly a straight line separating the two sets, making quite a lot of mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dbd813",
   "metadata": {},
   "source": [
    "### Overfitted model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27cc660",
   "metadata": {},
   "source": [
    "Now we will try just the opposite. Let's create a model that is so large that you end up learning practically all the examples by heart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ecc907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(1024, activation='relu', input_dim=2),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\")\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X, y, epochs=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e7608",
   "metadata": {},
   "source": [
    "Let's take a look at the evolution of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f2316",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a34670",
   "metadata": {},
   "source": [
    "We can observe the same problem as with regression. The model fits very well to the training set but poorly to the test set. Let us look at the regions it delimits in the space to discern between sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd8eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict(grid, verbose=0)\n",
    "\n",
    "plt.xlim(-2.5, 2.5)\n",
    "plt.ylim(-2.5, 2.5)\n",
    "plt.pcolor(gridx1, gridx2, probs.reshape(500, 500))\n",
    "plt.colorbar()\n",
    "plt.scatter(X[:,0], X[:,1], color=colors[y]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ee530",
   "metadata": {},
   "source": [
    "We can see that it is very specialized to try to correctly classify the examples.\n",
    "\n",
    "As an exercise, it would also be very good to play with different architectures to try to reach a correct classification. Thus, in addition to gaining more intuition in model training, it is useful to better understand the operation of `keras`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05096b5",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65fcd4",
   "metadata": {},
   "source": [
    "To conclude, we have performed two examples illustrating the problems of _underfitting_ and _overfitting_ in regression and classification problems, and how these issues impact the predictive ability of the models.\n",
    "\n",
    "Underfitting_ results in low prediction accuracy on training and test data, while _overfitting_ results in high accuracy on training data but low accuracy on test data. To avoid these problems, it is important to find a balance between the complexity of the model and its ability to generalize to new and unseen data.\n",
    "\n",
    "Proper feature selection, cross-validation, hyperparameter fitting and regularization are useful techniques to avoid _overfitting_ and _underfitting_ and improve the predictive ability of the models. These concepts will be discussed later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582efec2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
