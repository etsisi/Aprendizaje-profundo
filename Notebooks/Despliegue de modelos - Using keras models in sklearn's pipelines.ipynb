{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "source": [
    "# Using keras models in sklearn's pipelines\n",
    "\n",
    "One of the most powerful features of `scikit-learn` is the ability to create _pipelines_ to structure the data flow. These pipelines can be exported from one system to another like a black box.\n",
    "\n",
    "Keras models are not directly integrated into scikit-learn pipelines, but from the library itself there are two classes that act as _wrappers_ with the necessary methods for this integration to work: `keras.wrappers.scikit_learn.KerasClassifier(...)` and `keras.wrappers.scikit_learn.KerasRegressor(...)`, for classification and regression problems respectively.\n",
    "\n",
    "Let's import the necessary libraries that we will use in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f52ae25694875ed25b729e615a465a3f6ac8b4ad"
   },
   "source": [
    "We will now give a small demonstration of how one of these _wrappers_ would work. We will create a pipeline with a `scikit-learn` data normaliser and a classifier based on a multilayer perceptron of `keras`.\n",
    "\n",
    "To do this, we will first create some data that will serve as our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "fe3e4baf9a9e4949087f8eb79f44d72510ffc738"
   },
   "outputs": [],
   "source": [
    "X_train = np.random.random((1000, 3))\n",
    "y_train = np.eye(3)[np.random.choice(3, 1000)]\n",
    "X_test = np.random.random((100, 3))\n",
    "y_test = np.eye(3)[np.random.choice(3, 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create our model. The _wrapper_ requires a first parameter with the function that returns our model, so we encapsulate it in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "94557bff7d90843cd0a4ffd5db07485c0819e5e0"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(32, activation='relu', input_dim=3),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(3, activation='softmax'),\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='sgd',\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "classifier = tf.keras.wrappers.scikit_learn.KerasClassifier(\n",
    "    build_fn=build_model,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create our normaliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "2e97ed7731fcc955597280396a0431a2d84d1498"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scaler',scaler),\n",
    "    ('classifier',classifier),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to launch an adjustment (a training) in the pipeline and see how it performs with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test: 0.25999999046325684\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    classifier__batch_size=1000,\n",
    "    classifier__epochs=1000,\n",
    ")\n",
    "print(f'Accuracy on test: {pipeline.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
