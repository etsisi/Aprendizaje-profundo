{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010176d1",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "# Training Process Evaluation<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Authors: Alberto Díaz Álvarez<br>Last update: 2023-04-09</small></i></div> \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554fb61b",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba017a",
   "metadata": {},
   "source": [
    "It is important to monitor the training process of a model in order to evaluate and improve its performance. During the training of a neural network it is necessary to adjust numerous hyperparameters, such as learning rate, batch size, epochs number, topology of the network, etc.\n",
    "\n",
    "Tensorboard is a useful tool to monitor the training process, since it allows to graphically visualize various metrics that are collected during the training process, such as the evolution of the loss function, the network accuracy on training and validation data, the learning of the network features, among others.\n",
    "\n",
    "In addition, Tensorboard also allows to visualize the neural network structure and the distributions of the weights and activations of the layers, which can help to identify possible problems in the network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f243ba3",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d6d24f",
   "metadata": {},
   "source": [
    "The general objective of this notebook is to show how to use the **Tensorboard** tool to evaluate the training of neural network models.\n",
    "\n",
    "After showing how to start it inside our notebook (it is an external tool that can be used independently from the terminal), we will try to detect _exploding gradients_ and _vanishing gradients_ problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67582fcc",
   "metadata": {},
   "source": [
    "## Libraries and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde56587",
   "metadata": {},
   "source": [
    "Next we will import the libraries that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58374f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06541e93",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37fa74e",
   "metadata": {},
   "source": [
    "## Our sample model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aec0b7",
   "metadata": {},
   "source": [
    "For this example, we will create a model to solve the MNIST problem that we already saw in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a3e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train, x_test = x_train / 255, x_test / 255\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66883758",
   "metadata": {},
   "source": [
    "## Loading TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac64d8",
   "metadata": {},
   "source": [
    "TensorBoard is a **web-based tool** provided by TensorFlow. It is used **to visualize and debug the training process of machine learning models in TensorFlow**.\n",
    "\n",
    "It provides several features, including:\n",
    "\n",
    "- **Graph Visualization**: TensorBoard can visualize the computational graph of the model, which is helpful in understanding the structure of the model.\n",
    "- **Scalar Dashboard**: This feature allows users to visualize scalar values such as accuracy, loss, and learning rate over time during training. It helps in monitoring the progress of the model and detecting overfitting or underfitting.\n",
    "- **Histogram Dashboard**: TensorBoard can also display histograms of activations and weight distributions in the model.\n",
    "- **Projector Dashboard**: This feature is used to visualize high-dimensional data using t-SNE, a dimensionality reduction technique. It is helpful in understanding how the model is clustering the data.\n",
    "- **Profile Dashboard**: This feature provides a detailed analysis of the execution time and memory usage of different operations in the model. It helps in identifying performance bottlenecks in the model.\n",
    "\n",
    "Overall, TensorBoard is a powerful tool for understanding and debugging machine learning models in TensorFlow. To initialize it from a python notebook, simply load it as an external module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edcdc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f7e62",
   "metadata": {},
   "source": [
    "Then, we have to create a callback that will update the model values as we work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dac8b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f'logs/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f95be9",
   "metadata": {},
   "source": [
    "Once the module is loaded and the _callback_ that `tensorboard` will use is created, we start it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b053640",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b11c18",
   "metadata": {},
   "source": [
    "At this moment, tensorboard is listening in the log directory (where _loss_ and other metrics are being stored) and updating every 30 seconds.\n",
    "\n",
    "We can now train and see how the training of our model is evolving in `tensorboard`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13322dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=250, validation_split=0.1, verbose=0, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fcb1c2",
   "metadata": {},
   "source": [
    "## About the components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d8992f",
   "metadata": {},
   "source": [
    "We will now look at the main components that are available in TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f9d2c6",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7362f3",
   "metadata": {},
   "source": [
    "These are very complicated elements to follow as the complexity of the network increases.\n",
    "\n",
    "![An example graph](Images/graph.png \"Vista del grafo completo de nuestro ejemplo\")\n",
    "\n",
    "The way to get a useful view is to do some preliminary cleanup work to structure it.\n",
    "\n",
    "The graphs can help us enormously both to understand the model we are working with, and to detect errors in the topology of the model.\n",
    "\n",
    "Some keys to interpret the graph:\n",
    "\n",
    "1. Nodes with the same color imply that they belong to the same structure. Grays, however, indicate that each of the nodes is unique.\n",
    "2. Clicking on a node shows more details.\n",
    "3. There is a button that allows us to see dependencies of any node: `trace_input`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a9f294",
   "metadata": {},
   "source": [
    "### Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546708e",
   "metadata": {},
   "source": [
    "They are a special type of tensorflow operator. Just as there are operators such as algebraic operations (e.g. additions, subtractions, ...), there are operators that take as input a tensor of the graph and provide as output a set of \"summarized\" data.\n",
    "\n",
    "By default, there are automatically created summary operators (practically every graph that appears apart from the network in TensorBoard is an operator of this type), although we can create as many as we need. Once they are created, they will be dumped into the logs, which will be read from tensorboard.\n",
    "\n",
    "Now we will see some of the most common operators:\n",
    "\n",
    "* `tf.summary.scalar`: They write down individual values such as accuracy, loss, etc., displaying them in the form of a graph.\n",
    "* `tf.summary.image`: It displays an image, which is very useful to identify if the inputs are correct or if a generative model is producing images as expected.\n",
    "* `tf.summary.audio`: Similar to the previous operator, but for sound.\n",
    "* `tf.summary.histogram`: Useful for plotting the histogram of a non-scalar tensor, which shows how the distribution of the tensor value changes over time. In the case of DNN it is commonly used to check the distribution of weights and biases, helping to detect irregular behavior in the network parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696c3f97",
   "metadata": {},
   "source": [
    "## Vanishing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9c4e94",
   "metadata": {},
   "source": [
    "The gradient is a measure of the direction and magnitude of change in the loss function of the neural network, which is used to adjust the weights of the network connections during the training process.\n",
    "\n",
    "If the gradient is too small, this can lead to _vanishing gradients_ problems, where the weights of the network connections are updated in small jumps that can cause the network to stall at a local minimum and fail to learn more complex patterns.\n",
    "\n",
    "Let's do an exercise to identify how our model is suffering from a _vanishing gradients_ problem. We will first finish with the above `tensorboard` process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16095c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill $(ps -e | grep 'tensorboard' | awk '{print $1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54b6709",
   "metadata": {},
   "source": [
    "Now we will create a different model and launch a new tensorboard to evaluate the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a483ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(5, activation='tanh'),\n",
    "    tf.keras.layers.Dense(5, activation='tanh'),\n",
    "    tf.keras.layers.Dense(5, activation='tanh'),\n",
    "    tf.keras.layers.Dense(5, activation='tanh'),\n",
    "    tf.keras.layers.Dense(5, activation='tanh'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "log_dir = f'logs/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b83482f",
   "metadata": {},
   "source": [
    "Let's go with the training to see how it evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaca458",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=25, validation_split=0.1, verbose=0, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1608783",
   "metadata": {},
   "source": [
    "## Exploding gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4370b8",
   "metadata": {},
   "source": [
    "If the gradient is too large, this can lead to _exploding gradients_ problems, where the weights of the network connections are updated in large jumps that can make the network unable to converge to an optimal solution.\n",
    "\n",
    "We will start by terminating the previous tensorboard process so that we can subsequently launch a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c355d9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill $(ps -e | grep 'tensorboard' | awk '{print $1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a916a637",
   "metadata": {},
   "source": [
    "Since this problem is rare (although it does happen) in shallow networks, in our current example it is difficult to achieve the desired effect. We will try this by tricking the inputs to be larger than they should be, forcing the values traveling through the network to be very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1615e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = x_train * 10, y_train * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2d4c8",
   "metadata": {},
   "source": [
    "We will now train a model with these inputs to try to give us the expected outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558898f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(5, activation='relu', kernel_initializer='he_uniform'),\n",
    "    tf.keras.layers.Dense(5, activation='relu', kernel_initializer='he_uniform'),\n",
    "    tf.keras.layers.Dense(5, activation='relu', kernel_initializer='he_uniform'),\n",
    "    tf.keras.layers.Dense(5, activation='relu', kernel_initializer='he_uniform'),\n",
    "    tf.keras.layers.Dense(5, activation='relu', kernel_initializer='he_uniform'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "log_dir = f'logs/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a5b0f",
   "metadata": {},
   "source": [
    "We will now train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb814b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=25, validation_split=0.1, verbose=0, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05096b5",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65fcd4",
   "metadata": {},
   "source": [
    "This notebook has been intense, but in it we have seen the main differences of the two types of problems that we will encounter in deep learning problems: classification and regression. The models developed for these are very similar, varying basically in the output and its error calculation.\n",
    "\n",
    "Also, for the evaluation of these models we have presented some measures, some specific for classification and others for regression. There are some that we have not explained (e.g. cross-entropy) but we have preferred to stay with the most common ones. One good thing is that practically all frameworks include these implementations, probably much better than we can implement them ourselves. However, it is very important to know how we are measuring and what those measurements mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582efec2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
