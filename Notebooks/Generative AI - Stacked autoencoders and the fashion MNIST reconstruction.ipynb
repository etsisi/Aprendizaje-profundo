{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4480ff39",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Stacked autoencoders and the fashion MNIST reconstruction<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Authors: Alberto Díaz Álvarez<br>Last update: 2023-04-26</small></i></div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2489222",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17030ba",
   "metadata": {},
   "source": [
    "In the literature there are different components referred to as **stacked autoencoder**. In this notebook we will develop an example where the concept refers to those autoencoders whose encoder and decoder components are composed of several hidden layers before connecting to the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d1e17",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9e324",
   "metadata": {},
   "source": [
    "We will be using the Fashion MNIST dataset, which contains images of various clothing items, to train our stacked autoencoder model. The goal is to train the model to reconstruct the images with as little loss of information as possible.\n",
    "\n",
    "To make it more interesting, we will make use of the technique called \"tying weights\" to quicken the training of our model. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d401eac",
   "metadata": {},
   "source": [
    "## Libraries and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa528aec",
   "metadata": {},
   "source": [
    "Next we will import the libraries that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e97df",
   "metadata": {},
   "source": [
    "We will also configure some parameters to adapt the graphical presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca72f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fec89",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7223928",
   "metadata": {},
   "source": [
    "## Getting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d3a3bc",
   "metadata": {},
   "source": [
    "This time we will use the `fashion mnist` instead the basic `mnist`. This dataset is a collection of 70,000 grayscale images of clothing items, with 60,000 images used for training and 10,000 for testing. Each image is 28x28 pixels in size and belongs to one of 10 classes.\n",
    "\n",
    "It is a more challenging replacement for the original MNIST dataset, designed to have similar properties as the MNIST dataset, such as being well-balanced and easily accessible, but with more complexity and variety in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07075092",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255, x_test / 255\n",
    "\n",
    "print(f'Training shape: {x_train.shape} input')\n",
    "print(f'Test shape:     {x_test.shape} input')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9cb29",
   "metadata": {},
   "source": [
    "## The tying weights technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbc803a",
   "metadata": {},
   "source": [
    "It's a technique used to reduce the number of parameters in the model. In a traditional autoencoder, there are two sets of weights: the encoder weights and the decoder weights. When tying weights, the **decoder weights are constrained to be equal to the transpose of the encoder weights**. This means that the weights used to reconstruct the original input are the same weights used to compress the input, which reduces the number of parameters in the model (thus, helping to prevent overfitting and improving generalization).\n",
    "\n",
    "We are using `Dense` layers for our encoder and decoder components, but no `Dense` transposed layers exist in the Keras library. Therefore, we will create a custom layer that will reproduce this behavior in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc244555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransposedDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, from_layer, activation=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.from_layer = from_layer\n",
    "        activation = activation or self.from_layer.activation\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.biases = self.add_weight(\n",
    "            shape=self.from_layer.input_shape[-1],\n",
    "            initializer='zeros',\n",
    "        )\n",
    "        return super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        z = tf.matmul(x, self.from_layer.weights[0], transpose_b=True)\n",
    "        return self.activation(z + self.biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f63104",
   "metadata": {},
   "source": [
    "Bear in mind that it is not always necessary to tie the weights, and in some cases, it may not be beneficial. It depends on the specific architecture and problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f89f1f",
   "metadata": {},
   "source": [
    "## Building our stacked autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c68d85c",
   "metadata": {},
   "source": [
    "Now that we understand how autoencoders work and the concepts of the tying weights technique, lest's move on to building a stacked autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc7e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedAutoencoder(tf.keras.models.Model):\n",
    "    \"\"\"Represents a syacked autoencoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, layers, output_activation, name=None, *args, **kwargs):\n",
    "        super().__init__(*args, name=name, **kwargs)\n",
    "\n",
    "        # We calculate sizes and shapes of inputs, outputs and latent spaces\n",
    "        flatten_dim = None\n",
    "        if isinstance(input_dim, (list, tuple)):\n",
    "            flatten_dim = math.prod(input_dim)\n",
    "        elif isinstance(input_dim, int):\n",
    "            flatten_dim = input_dim\n",
    "            input_dim = (input_dim,)\n",
    "        else:\n",
    "            raise ValueError('Argument input_dim must be a tuple or an int')\n",
    "        \n",
    "        # Encoder definition: Connection between input and latent space\n",
    "        encoder_layers = [\n",
    "            tf.keras.layers.Dense(units, activation=tf.keras.layers.LeakyReLU(0.2))\n",
    "            for units in layers\n",
    "        ]\n",
    "        self.encoder = tf.keras.Sequential([tf.keras.layers.Flatten()], name='Encoder')\n",
    "        for dense in encoder_layers:\n",
    "            self.encoder.add(dense)\n",
    "        # Decoder definition: Connection between latent space and output\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            TransposedDense(dense) for dense in reversed(encoder_layers[1:])\n",
    "        ], name='Decoder')\n",
    "        self.decoder.add(TransposedDense(encoder_layers[0], output_activation))\n",
    "        self.decoder.add(tf.keras.layers.Reshape(input_dim))\n",
    "        \n",
    "        # Weights construction (just to have summary model's method working)\n",
    "        self.build((None, *input_dim))\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65b6b1d",
   "metadata": {},
   "source": [
    "As we can see, we have created a \"mirrored\" encoder as the decoder. However, the last layer uses a different activation function because it corresponds to the output of our network and, therefore, the values must be in the interval [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cdb9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = StackedAutoencoder(\n",
    "    input_dim=(28, 28),\n",
    "    layers=[64, 36,],\n",
    "    output_activation='sigmoid',\n",
    "    name='Stacked')\n",
    "sae.encoder.summary()\n",
    "sae.decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d0ab75",
   "metadata": {},
   "source": [
    "Well, let's now train our stacked autoencoder with the training set. We compile it Since it is a model, we have to compile it beforehand by specifying which loss function and which optimizer we are going to use. We will also be able to invoke model methods on it, such as `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0daf891",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "history = sae.fit(x_train, x_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a8ffb",
   "metadata": {},
   "source": [
    "Let's see how training has evolved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5833d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb4f2b2",
   "metadata": {},
   "source": [
    "The training looks pretty good. Let's see how some examples of the training set are encoded and decoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "images = np.array(random.sample(list(x_train), n))\n",
    "\n",
    "encoded = sae.encoder(images).numpy()\n",
    "decoded = sae.decoder(encoded).numpy()\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title('Original')\n",
    "\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded[i])\n",
    "    plt.title('Reconstructed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9944de34",
   "metadata": {},
   "source": [
    "And now, what will happen to data it has theoretically never seen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(random.sample(list(x_test), n))\n",
    "encoded = sae.encoder(images).numpy()\n",
    "decoded = sae.decoder(encoded).numpy()\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title('Original')\n",
    "\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded[i])\n",
    "    plt.title('Reconstructed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925fbfcc",
   "metadata": {},
   "source": [
    "Almost perfect, so we were able to compress the images from 28x28=784 bytes (they are monochrome) to 100 bytes, which is just over 78% compression ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43585490",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7defd88",
   "metadata": {},
   "source": [
    "We have explored the concept of stacked autoencoder as in using multiple layers in an autoencoder, and applied it to the Fashion MNIST dataset to reconstruct images of clothing items.\n",
    "\n",
    "We have also visualized the reconstruction performance of the stacked autoencoder on several examples from the test dataset and observed how the model learned to represent the images in a lower-dimensional space. Also, the tying weights between the encoder and decoder layers can reduce the number of parameters in the model and improve its performance. By using a two-layer stacked autoencoder with a bottleneck layer in between, we were able to achieve good reconstruction performance on the Fashion MNIST dataset.\n",
    "\n",
    "Overall, we found that stacked autoencoders can be a powerful tool for dimensionality reduction and image reconstruction, and that they can be particularly useful for applications where data compression and feature extraction are important, such as in image recognition and computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33005bde",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
