{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4480ff39",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Text classification with RNNs<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Author: Alberto Díaz Álvarez<br>Última actualización: 2023-05-11</small></i></div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2489222",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17030ba",
   "metadata": {},
   "source": [
    "In a previous notebook, we explored text classification with CNNs, which are well-suited for capturing local dependencies in text data. However, sometimes we need a more powerful model that can capture longer-term dependencies in the data. This is where RNNs come in.\n",
    "\n",
    "RNNs are specifically designed to model sequential data, which makes them them ideal for text classification tasks. Unlike traditional feedforward neural networks, which process inputs independently of each other, RNNs maintain a memory of previous inputs and use this information to make predictions about the current input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d1e17",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9e324",
   "metadata": {},
   "source": [
    "We will explore how to use RNNs for text classification in the same problem as in the previous notebook, where we classified Amazon reviews that users made about products. We will walk through a step-by-step example of building a text classification model with RNNs using the Keras library in Python.\n",
    "\n",
    "We will see that in reality the changes are minimal, since it is little more than changing one layer for another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d401eac",
   "metadata": {},
   "source": [
    "## Libraries and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa528aec",
   "metadata": {},
   "source": [
    "Next we will import the libraries that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import requests\n",
    "from shutil import unpack_archive\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3a5f84",
   "metadata": {},
   "source": [
    "We will also configure some parameters to adapt the graphic presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d522d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76e288",
   "metadata": {},
   "source": [
    "And create the necessary directories in case they have not been created previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c7f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('tmp', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fec89",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7223928",
   "metadata": {},
   "source": [
    "## Common parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9977840",
   "metadata": {},
   "source": [
    "We will keep the same global parameters to be able to compare both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many dimensions our word vectors have (50, 100, 200 or 300)\n",
    "EMBEDDING_DIM = 50\n",
    "# Our vocabulary max. size (the most frequent ones will be chosen)\n",
    "MAX_VOCAB_SIZE = 16384\n",
    "# Maximum sentence length\n",
    "MAX_SEQUENCE_LEN = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd945b",
   "metadata": {},
   "source": [
    "## Dataset processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aa0426",
   "metadata": {},
   "source": [
    "The process we will carry out will be the same as we did in the previous notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e5df57",
   "metadata": {},
   "source": [
    "### Dataset download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a1ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = 'https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Digital_Music_5.json.gz'\n",
    "DATASET_ZIP = 'tmp/Digital_Music_5.json.gz'\n",
    "\n",
    "# Download the remote file if it does not exist\n",
    "if not os.path.exists(DATASET_ZIP):\n",
    "    with open(DATASET_ZIP, 'wb') as f:\n",
    "        print(f'Downloading {DATASET_ZIP}...')\n",
    "        r = requests.get(DATASET_URL, verify=False)\n",
    "        f.write(r.content)\n",
    "        print('OK')\n",
    "\n",
    "corpus = pd.read_json(DATASET_ZIP, lines=True)\n",
    "corpus.dropna(subset=['overall', 'reviewText'], inplace=True)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdcdc82",
   "metadata": {},
   "source": [
    "### Preparing inputs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b853ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = corpus['reviewText'].astype(str).str.strip()\n",
    "print(f'Training input shape: {x_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933c217",
   "metadata": {},
   "source": [
    "### ... outputs ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f8283",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = corpus['overall'].astype(int).replace({\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 1,\n",
    "    4: 2,\n",
    "    5: 2,\n",
    "})\n",
    "print(f'Training output shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413fefc",
   "metadata": {},
   "source": [
    "### ... TextVectorization layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39847055",
   "metadata": {},
   "source": [
    "**Exercise: _Create the `vectorize_layer` layer, which will be an object of the `TextVectorization` class that will translate the words to integers. Remember to take into account the tokens for \"padding\" and for \"unknown\". An output similar to the following is expected_:**\n",
    "\n",
    "```\n",
    "Vocabulary length: 32770\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3221f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_VOCAB_SIZE + 2,\n",
    "    output_sequence_length=MAX_SEQUENCE_LEN,\n",
    "    name='vectorization',\n",
    ")\n",
    "vectorize_layer.adapt(x_train)\n",
    "\n",
    "print(f'Vocabulary length: {len(vectorize_layer.get_vocabulary())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dac35d",
   "metadata": {},
   "source": [
    "### ... and embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2efd1ae",
   "metadata": {},
   "source": [
    "Which consist in download ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "GLOVE_FILE = 'tmp/glove.6B.zip'\n",
    "\n",
    "# Download the compressed GloVe dataset (if you don't already have it)\n",
    "if not os.path.exists(GLOVE_FILE):\n",
    "    print('Downloading ...', end='')\n",
    "    with open(GLOVE_FILE, 'wb') as f:\n",
    "        r = requests.get(GLOVE_URL, allow_redirects=True)\n",
    "        f.write(r.content)\n",
    "    print('OK')\n",
    "\n",
    "# Unzip it in the directory 'glove'.\n",
    "print('Unpacking ...', end='')\n",
    "unpack_archive(GLOVE_FILE, 'tmp')\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ece6e",
   "metadata": {},
   "source": [
    "... build weights (vectors) matrix ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d05ac88",
   "metadata": {},
   "source": [
    "**Exercise: _Construct the weights matrix of our vocabulary words using the GLoVe weights. We expect an output similar to the following_:**\n",
    "\n",
    "```\n",
    "Loading GloVe 50-d embedding... done (400000 word vectors loaded)\n",
    "Creating embedding matrix with GloVe vectors... done (4449 words unassigned)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b548d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Loading GloVe {EMBEDDING_DIM}-d embedding... ', end='')\n",
    "word2vec = {}\n",
    "with open(f'tmp/glove.6B.{EMBEDDING_DIM}d.txt') as f:\n",
    "    for line in f:\n",
    "        word, vector = line.split(maxsplit=1)\n",
    "        word2vec[word] = np.fromstring(vector,'f', sep=' ')\n",
    "print(f'done ({len(word2vec)} word vectors loaded)')\n",
    "\n",
    "print('Creating embedding matrix with GloVe vectors... ', end='')\n",
    "# Our newly created embedding: a matrix of zeros\n",
    "embedding_matrix = np.zeros((MAX_VOCAB_SIZE + 2, EMBEDDING_DIM))\n",
    "\n",
    "ko_words = 0\n",
    "for i, word in enumerate(vectorize_layer.get_vocabulary()):\n",
    "    if word == '[UNK]':\n",
    "        # The second word is for an unknown token, in glove is 'unk'\n",
    "        word = 'unk'\n",
    "\n",
    "    # Get the word vector and overwrite the row in its corresponding position\n",
    "    word_vector = word2vec.get(word)\n",
    "    if word_vector is not None:\n",
    "        embedding_matrix[i] = word_vector\n",
    "    else:\n",
    "        ko_words += 1\n",
    "print(f'done ({ko_words} words unassigned)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbce887",
   "metadata": {},
   "source": [
    "... and load the matrix inside an `Embedding` layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10802af",
   "metadata": {},
   "source": [
    "**Exercise: _Create the `embedding_layer` layer, which will be an object of the `Embedding` class that will translate the tokens to word vectors. We expect an output similar to the following_:**\n",
    "\n",
    "```\n",
    "<tf.Tensor: shape=(2, 50), dtype=float32, numpy=\n",
    "array([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
    "       [-7.9149e-01,  8.6617e-01,  1.1998e-01,  9.2287e-04,  2.7760e-01,\n",
    "        -4.9185e-01,  5.0195e-01,  6.0792e-04, -2.5845e-01,  1.7865e-01,\n",
    "         2.5350e-01,  7.6572e-01,  5.0664e-01,  4.0250e-01, -2.1388e-03,\n",
    "        -2.8397e-01, -5.0324e-01,  3.0449e-01,  5.1779e-01,  1.5090e-02,\n",
    "        -3.5031e-01, -1.1278e+00,  3.3253e-01, -3.5250e-01,  4.1326e-02,\n",
    "         1.0863e+00,  3.3910e-02,  3.3564e-01,  4.9745e-01, -7.0131e-02,\n",
    "        -1.2192e+00, -4.8512e-01, -3.8512e-02, -1.3554e-01, -1.6380e-01,\n",
    "         5.2321e-01, -3.1318e-01, -1.6550e-01,  1.1909e-01, -1.5115e-01,\n",
    "        -1.5621e-01, -6.2655e-01, -6.2336e-01, -4.2150e-01,  4.1873e-01,\n",
    "        -9.2472e-01,  1.1049e+00, -2.9996e-01, -6.3003e-03,  3.9540e-01]],\n",
    "      dtype=float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953aff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=embedding_matrix.shape[0],\n",
    "    output_dim=embedding_matrix.shape[1],\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LEN,\n",
    "    trainable=False,\n",
    "    name='Embedding',\n",
    ")\n",
    "\n",
    "embedding_layer(np.array([0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6eedd5",
   "metadata": {},
   "source": [
    "## Classification model based on recurrent neural networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0758b",
   "metadata": {},
   "source": [
    "And now, instead of using CNNs, we will use RNNs, networks that are designed for this type of problem. In this case, the dimension set is performed by the `TextVectorization` layer (which converts the text into sequences of integers of length $T$) and the Embedding layer (which converts each integer into a vector of $D$ dimensions), converting the input into a tensor with the form $N \\times T \\times D$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c6968",
   "metadata": {},
   "source": [
    "**Exercise: _Create a recurrent model that takes as input a string, vectorizes it into word ids, then into word vectors and finally, passes it through one or more recurrent layers to end up classifying it into one of the three scores_.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    vectorize_layer,\n",
    "    embedding_layer,\n",
    "    tf.keras.layers.GRU(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['sparse_categorical_accuracy'],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ee23e3",
   "metadata": {},
   "source": [
    "Although the number of parameters is similar, increasing the number of units in a recurrent unit does not increase the number of parameters in our model very much. However, it greatly increases the training time. Therefore, our model will not be able to achieve results as good as the previous ones.\n",
    "\n",
    "Let's train the model and hope for the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd1f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade383c",
   "metadata": {},
   "source": [
    "Let's take a look at the training progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d202e83a",
   "metadata": {},
   "source": [
    "Now let's see how it interprets the sentiment of a good, fair and bad review extracted from the amazon website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "good = \"My nephew is on the autism spectrum and likes to fidget with things so I knew this toy would be a hit. Was concerned that it may not be \\\"complex\\\" enough for his very advanced brain but he really took to it. Both him (14 yrs) and his little brother (8 yrs) both enjoyed playing with it throughout Christmas morning. I'm always happy when I can find them something unique and engaging.\"\n",
    "poor = \"I wasn't sure about this as it's really small. I bought it for my 9 year old grandson. I was ready to send it back but my daughter decided it was a good gift so I'm hoping he likes it. Seems expensive for the price though to me.\"\n",
    "evil = \"I just wanted to follow up to say that I reported this directly to the company and had no response. I have not gotten any response from my review. The level of customer service goes a long way when an item you purchase is defective and this company didn’t care to respond. No I am even more Leary about ordering anything from this company. I never asked for a refund or replacement since I am not able to return it. I’m just wanted to let them know that this was a high dollar item and I expected it to be a quality item. Very disappointed! I bought this for my grandson for Christmas. He loved it and played with it a lot. My daughter called to say that the stickers were peeling on the corners. I am not able to take it from my grandson because he is autistic and wouldn’t understand. I just wanted to warn others who are wanting to get this. Please know that this is a cool toy and it may not happen to yours so it is up to you.\"\n",
    "\n",
    "probabilities = model.predict([good, poor, evil], verbose=0)\n",
    "print(f'Good was classified as {np.argmax(probabilities[0])}')\n",
    "print(f'Poor was classified as {np.argmax(probabilities[1])}')\n",
    "print(f'Evil was classified as {np.argmax(probabilities[2])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43585490",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7defd88",
   "metadata": {},
   "source": [
    "We have explored how to use RNNs for text classification and compared them to CNNs. While both architectures can be effective for text classification, they have some key differences and trade-offs.\n",
    "\n",
    "One major difference between RNNs and CNNs is that RNNs are better suited for capturing long-term dependencies in text data, while CNNs are better suited for capturing local dependencies. This makes RNNs a good choice for tasks such as sentiment analysis or language translation, where the context of a word or phrase can be crucial for determining its meaning.\n",
    "\n",
    "However, RNNs can be more computationally expensive to train than CNNs, and they can also suffer from the vanishing gradient problem, which can make it difficult for the model to remember information from earlier in the input sequence. On the other hand, CNNs can be faster to train and can scale well to larger datasets. They are particularly well-suited for tasks such as text classification or image recognition, where local features are important.\n",
    "\n",
    "In the end, the choice between RNNs and CNNs for text classification will depend on the specific requirements of the task, the characteristics of the data, and the resources available for training and testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33005bde",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
