{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Creación de capas y modelos personalizados<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto Díaz Álvarez<br>Última actualización: 2023-04-10</small></i></div>\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede que no sea hoy. Ni tampoco mañana. Pero en algún momento nos veremos en la tesitura de tener que crear una función de pérdida o un optimizador. O incluso una capa como subclase de `Layer` o un modelo como subclase de `Model`. Y en ese momento descubriremos que no todo era tan fácil como parecía, sobre todo a la hora de exportar e importar de nuevo dicho modelo.\n",
    "\n",
    "En este _notebook_ aprenderemos cómo tenemos que trabajar para tener componentes personalizados de Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación importaremos las librerías que se usarán a lo largo del _notebook_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#from collections import deque\n",
    "#import os\n",
    "#import pathlib\n",
    "#import random\n",
    "#import shutil\n",
    "#from typing import Callable, List,  NamedTuple, Sequence, SupportsFloat, Union\n",
    "#import uuid\n",
    "\n",
    "#import gymnasium as gym\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import pandas as pd\n",
    "#import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asímismo, configuramos algunos parámetros para adecuar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (15, 8),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entorno de simulación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar el entorno de simulación `LunarLander` de `gymnasium`ya que lo conocemos bastante bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestra primera acción será ver el espacio de observaciones y de acciones de nuestro entorno. Tal y como hemos visto durante la teoría:\n",
    "\n",
    "- Espacio de observaciones: Todos los posibles estados de nuestro entorno que el agente es capaz de ver. En algunos casos se corresponderá con el espacio de estados de nuestro problema, pero no siempre tiene por qué ser así.\n",
    "- Espacio de acciones: Todas las acciones posibles de nuestro agente sobre el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Input: {env.observation_space}')\n",
    "print(f'Output: {env.action_space}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretamente, los espacios de observaciones y acciones son de tipo `Box` y `Discrete` respectivamente. Esto son clases concretas del entorno `gymnasium`, pero lo importante es que hay 8 entradas que nos indican el estado de la nave en el entorno y cuatro posibles acciones a realizar.\n",
    "\n",
    "En _Deep $Q$-learning_ los espacios de observaciones y acciones se corresponderán con las entradas y las salidas del modelo que sustituye a la red Q.\n",
    "\n",
    "Ahora, antes de seguir, vamos a el entorno que hemos creado para no tener problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elementos auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como la implementación será un poco más larga que el _notebook_ anterior, haremos uso de una serie de elementos auxiliares que nos ayudarán ha hacer el código más manejable.\n",
    "\n",
    "Comenzamos definiendo la clase `Transition`, que representará la transición que ocurre de un estado $s_t$ a un estado $s_{t+1}$. Dicha transición contendrá la información de qué acción provocó la transición, cuál fue la recompensa de haberla llevado a cabo y si se ha terminado, ya sea por llegar a un estado terminal (p.ej. llegar al destino o morir) o por cualquier otra razón (p.ej llegar a un determinado límite de tiempo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(NamedTuple):\n",
    "    \"\"\"Representa la transición de un estado al siguiente\"\"\"\n",
    "    prev_state: gym.core.ObsType  # Estado origen de la transición\n",
    "    next_state: gym.core.ObsType  # Estado destino de la transición\n",
    "    action: gym.core.ActType      # Acción que provocó esta transición\n",
    "    reward: SupportsFloat         # Recompensa obtenida\n",
    "    terminated: bool              # Si se ha llegado a un estado terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También crearemos un objeto que almacenará la memoria de experiencias vividas por nuestro agente. Estas experiencias se corresponderán con la información de todas las transiciones por las que han pasados los estados en los que se ha ido encontrando.\n",
    "\n",
    "Se implementará como un _buffer_ de un tamaño determinado, por lo que llegará un momento que las antiguas experiencias se irán eliminando para dar paso a las nuevas. TODO: QUIZÁ EXPLICAR UN POQUITO MÁS CÓMO SE VA A USAR DURANTE EL ENTRENAMIENTO, Y AÑADIR UN DIBUJITO PARA QUE SE VEA CÓMO FUNCIONA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuestro agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear un agente que tratará de resolver este problema en el entorno existente. Intentaremos que se parezca en la medida de lo posible al que hemos visto resolviendo el problema del 1Dungeon, pero la implementación será un poco más grande (que no complicada, sólo grande).\n",
    "\n",
    "Comenzaremos por el principio, un agente que se desenvuelve en un entorno.\n",
    "\n",
    "```python\n",
    "class Agent:\n",
    "    def __init__(self, *, env: gym.Env):\n",
    "        self.env = env\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tareas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si recordamos, se denominaba «tarea» a una instancia de un problema, que podía tratarse de una tarea episódica (tiene un comienzo y un fin) o continua (tiene comienzo pero no tiene fin).\n",
    "\n",
    "El agente, para aprender, ejecutará tareas una detrás de otra. Cada una de ellas constará de un ciclo constante de percibir el entorno, decidir la acción a ejecutar y ejecutar dicha acción sobre el entorno. Esto lo gestionaremos a través del método `task` de nuestro agente.\n",
    "\n",
    "```python\n",
    "class Agent:\n",
    "    def __init__(self, *, env: gym.Env):\n",
    "        # ...\n",
    "        self.current_state = None\n",
    "\n",
    "    def task(self, max_iterations: int=None) -> float:\n",
    "        self.current_state, _ = env.reset()\n",
    "\n",
    "        max_iterations = max_iterations or np.inf\n",
    "        reward = 0\n",
    "        running = True\n",
    "        while running and max_iterations > 0:\n",
    "            # Por si acaso tenemos un número máximo de acciones por tarea\n",
    "            max_iterations -= 1\n",
    "            # Ciclo de percibir-decidir-actuar\n",
    "            perception = self.perceive()\n",
    "            action = self.decide(perception)\n",
    "            transition = self.act(action)\n",
    "            # Añadimos la recompensa de la acción a la recompensa de la tarea\n",
    "            reward += transition.reward\n",
    "            # Condición de parada\n",
    "            running = not transition.terminated\n",
    "        return reward\n",
    "```\n",
    "\n",
    "Como vemos, la tarea reinicia el entorno para comenzar la ejecución, asigna el estado inicial a un atributo del objeto llamado `current_state` y procede a realizar tantas iteraciones de percibir-decidir-actuar como sean necesarias hasta llegar a una solución (i.e. la última transición ha llegado a un estado de finalización) o a un máximo de iteraciones (controlado mediante el argumento `max_iterations`.\n",
    "\n",
    "Ahora nos quedaría definir cómo se percibe, decide y actúa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percibir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro problema vamos a usar como percepción exclusivamente el estado devuelto por el entorno, aunque en otros casos podría usarse otra información adicional (por ejemplo el número de iteraciones que llevamos dentro del episodio para que aprenda a tener prisa si se está acabando el tiempo, por ejemplo).\n",
    "\n",
    "```python\n",
    "class Agent:\n",
    "    # ...\n",
    "    \n",
    "    def perceive(self):\n",
    "        return self.current_state\n",
    "```\n",
    "\n",
    "Eso sí, tenemos que tener cuidado para que, cuando actuemos sobre el entorno y éste nos devuelva el nuevo estado, almacenarlo en este atributo. Así, en la siguiente vuelta del bucle cuando toque percibir tendremos disponible el nuevo estado para decidir sobre él. Y hablando de decidir..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decidir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando decimos «decidir» nos referimos a determinar en cada momento queé acción ejecutaremos dada una observación. Pero claro, ya vimos anteriormente que escoger siempre la mejor acción no tiene por qué ser la mejor estrategia de acción. Es más, a menudo las recompensas a largo plazo suelen ser mejores que las de corto plazo.\n",
    "\n",
    "Por ello, volveremos a usar la estrategia _$\\epsilon$-greedy_ con decaimiento, de tal manera que no siempre escogerá la mejor acción, sino que variará dependiendo de en qué momento del entrenamiento estemos, comenzando con una estrategia más aleatoria al principio y más conservadora al final.\n",
    "\n",
    "Tenemos que hacer, eso sí, una pequeña modificación sobre el método `task` para que reciba el parámetro `epsilon`, así éste será pasado al método `decide` que escojerá la acción a ejecutar.\n",
    "\n",
    "```python\n",
    "class Agent:\n",
    "    def __init__(self, *, env: gym.Env):\n",
    "        # ...\n",
    "        self.model = ??\n",
    "\n",
    "    def task(self, epsilon: float, max_iterations: int=None) -> float:\n",
    "        # ...\n",
    "        while running and max_iterations > 0:\n",
    "            # ...\n",
    "            action = self.decide(perception, epsilon)\n",
    "            # ...\n",
    "        return reward\n",
    "\n",
    "    # ...\n",
    "\n",
    "    def decide(self, perception: np.ndarray, epsilon: float) -> np.ndarray:\n",
    "        if random.random() < epsilon:\n",
    "            # Selección aleatoria\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            # Selección voraz\n",
    "            perception = perception[np.newaxis, ...]\n",
    "            q_values = self.model.predict(perception, verbose=0)[0]\n",
    "            return np.argmax(q_values)\n",
    "```\n",
    "\n",
    "Vemos que en el caso de la selección voraz, es decir, la selección de la mejor acción, obtiene los _$q$-values_ de todas las opciones posibles de un modelo predictivo, escogiendo la mejor acción a partir del valor más alto. Más adelante veremos de dónde sale ese modelo. Por ahora vamos a terminar implementando el método dectinado a actuar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actuar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este método se ejecutará la acción decidida. En nuestro problema en concreto delegamos totalmente en el entorno en cuestión, pero podría ser que el agente tuviese lógica adicional que hiciese que la transición o la acción pudiese variar.\n",
    "\n",
    "En nuestro caso, la implementación es muy sencilla:\n",
    "\n",
    "```python\n",
    "class Agent:\n",
    "    # ...\n",
    "\n",
    "    def act(self, action):\n",
    "        next_state, reward, terminated, _, _ = self.env.step(action)\n",
    "        self.current_state = next_state\n",
    "        \n",
    "        return Transition(\n",
    "            prev_state=self.current_state,\n",
    "            next_state=next_state,\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            terminated=terminated,\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las características que tendrá nuestro agente es la memoria. Durante la ejecución del escenario, irá teniendo experiencias, representadas como transiciones entre estados. Esta memoria es la que se usará durante la fase de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"Representa la memoria de un agente.\n",
    "\n",
    "    Concretamente, almacenará las últimas n transiciones realizadas en\n",
    "    el entorno. El tamaño de la memoria se establecerá en el momento de\n",
    "    crear la memoria del mismo.\n",
    "\n",
    "    La memoria guarda las transiciones de manera ordenada, y se podrá\n",
    "    acceder a ellos por índice, de manera que el recuerdo más lejano\n",
    "    estará en la posición 0 y el más reciente en la posición -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size: int):\n",
    "        \"\"\"Inicializa el objeto.\n",
    "\n",
    "        :param size: El tamaño máximo de la memoria del agente.\"\"\"\n",
    "        self.max_size = int(size)\n",
    "        self.transitions: deque = deque(maxlen=self.max_size)\n",
    "\n",
    "    def append(self, transition: Transition):\n",
    "        \"\"\"Añade un nuevo recuerdo a la memoria del agente.\n",
    "\n",
    "        :param transition: La transición a recordar.\"\"\"\n",
    "        self.transitions.append(transition)\n",
    "\n",
    "    def batch(self, n: int) -> List[Transition]:\n",
    "        \"\"\"Devuelve n recuerdos aleatorios de la memoria.\n",
    "\n",
    "        :param n: El número de recuerdos aleatorios a devolver. Si es\n",
    "            superior al número de recuerdos totales devolverá todos los\n",
    "            recuerdos almacenados.\n",
    "        :returns: La lista de transiciones.\n",
    "        \"\"\"\n",
    "        n = min(len(self.transitions), n)\n",
    "        return random.sample(self.transitions, n)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"El número de recuerdos que contiene esta memoria.\n",
    "\n",
    "        :returns: Un entero mayor o igual a 0.\n",
    "        \"\"\"\n",
    "        return len(self.transitions)\n",
    "\n",
    "    def __getitem__(\n",
    "            self,\n",
    "            key: Union[int, slice]\n",
    "    ) -> Union[Transition, Sequence[Transition]]:\n",
    "        \"\"\"Devuelve el/los elemento/s especificados.\n",
    "\n",
    "        :param key: El argumento que indica los elementos. Puede ser un\n",
    "            entero normal o un slice.\n",
    "        :returns: El/los elemento/s especificados por el índice.\n",
    "        \"\"\"\n",
    "        return self.transitions.__getitem__(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora actualizaremos nuestro agente para que tenga una memoria de acciones.\n",
    "\n",
    "```python\n",
    "class Agent:\n",
    "    def __init__(self, *, env: gym.Env, memory_size: int=1e5):\n",
    "        # ...\n",
    "        self.memory = Memory(size=memory_size)\n",
    "\n",
    "    # ...\n",
    "\n",
    "    def act(self, action):\n",
    "        # ...\n",
    "        self.memory.append(Transition(\n",
    "            prev_state=self.current_state,\n",
    "            next_state=next_state,\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            terminated=terminated,\n",
    "        ))\n",
    "\n",
    "        return self.memory[-1]\n",
    "```\n",
    "\n",
    "Básicamente actualizamos la memoria justo antes de devolver la transición efectuada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de comportamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos dicho, la tabla $Q$ de la estrategia $Q$-learning se sustituye por un modelo de redes neuronales en _dee $Q$-learning_. Por tanto, habilitaremos al agente para que admita un modelo que será en el que almacenará su experiencia.\n",
    "\n",
    "Para que sea más versátil, nuestro agente admitirá un modelo de tres formas diferentes:\n",
    "\n",
    "1. Como función que devuelve un modelo.\n",
    "1. Directamente como modelo\n",
    "1. Como cadena de caracteres representando el _path_ donde se encuentra el modelo.\n",
    "\n",
    "```python\n",
    "class Agent:\n",
    "    def __init__(\n",
    "            self, *,\n",
    "            env: gym.Env,\n",
    "            model: Union[Callable[[gym.Env], tf.keras.Model], tf.keras.Model, str],\n",
    "            memory_size: int = 1e5,\n",
    "    ):\n",
    "        # ...\n",
    "        if callable(model):\n",
    "            self.model = model(self.env)\n",
    "        elif isinstance(model, tf.keras.models.Model):\n",
    "            self.model = tf.keras.models.clone_model(model)\n",
    "        elif isinstance(model, str):\n",
    "            self.model = tf.keras.models.load_model(model)\n",
    "        else:\n",
    "            raise ValueError('Valid models are a function, a model or a path')\n",
    "```\n",
    "\n",
    "Como vemos, este modelo tendrá como entrada el estado del entorno y como salida los cuatro _$q$-values_ asociados a cada acción. Sin embargo, aunque tenemos el modelo, aún no hemos hablado de como entrenarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenando nuestro modelo de comportamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea detrás del aprendizaje en este tipo de algoritmos es que el modelo de entrena cada $x$ pasos a partir de la información recabada del pasado, que está almacenada en memoria.\n",
    "\n",
    "Esta fase nos obligará (de nuevo) a actualizar el método `act`, así como la creación de algunos argumentos más:\n",
    "\n",
    "- `train_steps_rate`: Cada cuantas iteraciones entrenamos el modelo.\n",
    "- `batch_size`: Con cuantas transiciones entrenamos de todas las diponibles en memoria.\n",
    "- `gamma`: El factor de ajuste que sube o baja la recompensa futura.\n",
    "\n",
    "Además, creamos un nuevo atributo, `current_step` que nos indica en qué paso del entrenamiento nos encontramos. Con esto explicado, veamos los cambios:\n",
    "\n",
    "```python\n",
    "class Agent:\n",
    "    def __init__(\n",
    "            self, *,\n",
    "            env: gym.Env,\n",
    "            model: Union[Callable[[gym.Env], tf.keras.Model], tf.keras.Model, str],\n",
    "            train_steps_rate,\n",
    "            batch_size=32,\n",
    "            memory_size: int = 1e5,\n",
    "            gamma=0.99,\n",
    "    ):\n",
    "        # ...\n",
    "        self.train_steps_rate = train_steps_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.current_step = 0\n",
    "    # ...\n",
    "\n",
    "    def task(self, epsilon, max_iterations=None):\n",
    "        self.current_state, _ = env.reset()\n",
    "        self.current_step = 0\n",
    "        # ...\n",
    "        while running and self.current_step < max_iterations:\n",
    "            # ...\n",
    "            running = not transition.terminated\n",
    "            self.current_step += 1  # Ya no hace falta decrementar max_iterations\n",
    "        return reward\n",
    "\n",
    "    # ...\n",
    "\n",
    "    def act(self, action):\n",
    "        # ...\n",
    "        self.memory.append(Transition(\n",
    "            prev_state=self.current_state,\n",
    "            next_state=next_state,\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            terminated=terminated,\n",
    "        ))\n",
    "\n",
    "        if len(self.memory) > self.batch_size and self.current_step % self.train_steps_rate == 0:\n",
    "            batch = self.memory.batch(n=self.batch_size)\n",
    "            inputs = np.array([t.prev_state for t in batch])\n",
    "            labels = self.compute_labels(batch)\n",
    "            self.model.fit(\n",
    "                x=inputs,\n",
    "                y=labels,\n",
    "                batch_size=self.batch_size,\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "        return self.memory[-1]\n",
    "```\n",
    "\n",
    "Como podemos observar, la actualización del modelo salta cada vez que el número de iteraciones es múltiplo de `train_steps_rate`. Pero hay un detalle que no hemos explicado aún: las _labels_. Lo explicaremos después de hablar de la técnica del doble _deep $Q$-learning_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doble _Deep $Q$-learning_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de entrenar se realiza una inferencia antes del cálculo y propagación hacia atrás del error. Cuando usamos el mismo modelo para calcular $Q(s, a)$ y para calcular el _$Q$-value_ de la mejor acción del siguiente estado estamos tendiendo a sobreestimar la recompensa esperada (para más información, el artículo [Double Q-Learning & Double DQN with Python and TensorFlow](https://rubikscode.net/2021/07/20/introduction-to-double-q-learning/) lo describe más en detalle)..\n",
    "\n",
    "La forma de resolver este problema es manteniendo dos modelos, los cuales se suelen denominar $\\Theta$ y $\\Theta'$\n",
    "\n",
    "- $\\theta$: Modelo principal del que obtendremos las acciones a realizar.\n",
    "- $\\theta'$: Modelo objetivo, a partir del cual calcularemos los valores objetivo.\n",
    "\n",
    "Los pesos del modelo principal $\\theta$ se copian al modelo objetivo $\\theta'$ cada $x$ pasos.\n",
    "\n",
    "Empíricamente se observa que el uso del doble $q$-learning conduce a valores $Q$ más realistas y, por tanto, a mejores modelos. Nosotros, a los modelos $\\theta$ y $\\theta'$ los denominaremos `model` y `target_model` respectivamente.\n",
    "\n",
    "DARLE UNA VUELTA A LA WIKIPEDIA PORQUE HE VISTO QUE SE PUEDE COMPLEMENTAR UN POCO LA INFORMACIÓN: https://es.wikipedia.org/wiki/Q-learning\n",
    "\n",
    "Ahora vamos a ver los cambios necesarios que tenemos que realizar en nuestro agente para hacer uso de este esquema de aprendizaje.\n",
    "\n",
    "```python\n",
    "class Agent:\n",
    "    def __init__(\n",
    "            self, *,\n",
    "            env: gym.Env,\n",
    "            model: Union[Callable[[gym.Env], tf.keras.Model], tf.keras.Model, str],\n",
    "            target_model_update_rate,\n",
    "            train_steps_rate,\n",
    "            batch_size=32,\n",
    "            memory_size: int = 1e5,\n",
    "            gamma=0.99,\n",
    "    ):\n",
    "        # ...\n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "        self.target_model_update_rate = target_model_update_rate\n",
    "\n",
    "    # ...\n",
    "\n",
    "    def act(self, action):\n",
    "        # ...\n",
    "        self.memory.append(Transition(\n",
    "            prev_state=self.current_state,\n",
    "            next_state=next_state,\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            terminated=terminated,\n",
    "        ))\n",
    "\n",
    "        if self.current_step % self.target_model_update_rate == 0:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        if len(self.memory) > self.batch_size and self.current_step % self.train_steps_rate == 0:\n",
    "            # ...\n",
    "```\n",
    "\n",
    "Como podemos ver, hemos creado un clon del modelo del agente con la misma arquitectura y pesos para que, cada vez que llevamos cierto número de iteraciones determinadas por el valor de `target_model_update_rate`, volquemos los pesos del modelo base al modelo _target_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de las _labels_ para el modelo de comportamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora sí, vamos al asunto de la predicción de los valores. Éstos se tienen que corresponder con los _$q$-values_ óptimos, pero no los conocemos (evidentemente, si no, no estaríamos haciendo todo esto). El modelo terminará aprendiéndolos, por lo que crearemos un conjunto de _$q$-values_ que aproximen las recompensas obtenidas en algunas de las transiciones que hicimos en el pasado (el _batch_ extraído aleatoriamente de la memoria.\n",
    "\n",
    "Para predecir el valor de la mejor acción del estado futuro usaremos el valor que da `target_model` a la mejor acción predicha por el modelo base.\n",
    "\n",
    "Con esto ya tenemos una estimación de la mejor acción a elegir para el estado actual, por lo que pondremos a 0 los valores del resto de acciones para que el modelo trate de aproximarse a dicho valor. Veamos una posible implementación del método ``, que es el que usaremos para calcular dichas estimaciones objetivo:\n",
    "\n",
    "```python\n",
    "class Agent:\n",
    "    # ...\n",
    "\n",
    "    def compute_labels(self, transitions):\n",
    "        # Tomamos los estados destino de las transiciones del batch\n",
    "        next_states = np.array([t.next_state for t in transitions])\n",
    "\n",
    "        # Calculamos los q-values tanto del modelo base como del objetivo\n",
    "        q_base = self.model.predict(next_states, verbose=0)\n",
    "        q_target = self.target_model.predict(next_states, verbose=0)\n",
    "\n",
    "        targets = []\n",
    "        for i, transition in enumerate(transitions):\n",
    "            # Seleccionamos la mejor acción\n",
    "            best_action_base = np.argmax(q_base[i])\n",
    "            best_action_target = q_target[i][best_action_base]\n",
    "\n",
    "            # Calculamos su recompensa asociada\n",
    "            reward = transition.reward\n",
    "            if not transition.terminated:\n",
    "                reward += self.gamma * best_action_target\n",
    "\n",
    "            # Creamos la label de entrenamiento asociada a este ejemplo\n",
    "            target_vector = [0 for _ in range(self.env.action_space.n)]\n",
    "            target_vector[transition.action] = reward\n",
    "            targets.append(target_vector)\n",
    "\n",
    "        return np.array(targets)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versión final del agente\n",
    "\n",
    "Nuestro agente está terminado. La versión final después de todo lo explicado (que no es poco) es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "            self, *,\n",
    "            env: gym.Env,\n",
    "            model: Union[Callable[[gym.Env], tf.keras.Model], tf.keras.Model, str],\n",
    "            target_model_update_rate,\n",
    "            train_steps_rate,\n",
    "            batch_size=32,\n",
    "            memory_size: int = 1e5,\n",
    "            gamma=0.99,\n",
    "    ):\n",
    "        \"\"\"Inicializa el objeto.\n",
    "        :param model: El modelo del objeto. Puede ser una función que devuelva\n",
    "            un nuevo modelo (compilado), un objeto de `tf.keras.Model` ya\n",
    "            existente (en cuyo caso se clonará) o una cadena con un path\n",
    "            válido, en cuyo caso se cargará de disco.\n",
    "        \"\"\"\n",
    "        # El entorno en el que vamos a trabajar\n",
    "        self.env = env\n",
    "\n",
    "        # El modelo de aprendizaje de nuestro agente\n",
    "        if callable(model):\n",
    "            self.model = model(self.env)\n",
    "        elif isinstance(model, tf.keras.models.Model):\n",
    "            self.model = tf.keras.models.clone_model(model)\n",
    "        elif isinstance(model, str):\n",
    "            self.model = tf.keras.models.load_model(model)\n",
    "        else:\n",
    "            raise ValueError('Valid models are a function, a model or a path')\n",
    "\n",
    "        # El clon del modelo que hace las veces del modelo objetivo\n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "        self.target_model_update_rate = target_model_update_rate\n",
    "\n",
    "        # La memoria de transiciones que mantendrá el agente\n",
    "        self.memory = Memory(size=memory_size)\n",
    "\n",
    "        self.train_steps_rate = train_steps_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Indica en el estado que se encuentra el agente\n",
    "        self.current_state = None\n",
    "        # Para llevar la cuenta de cada cuánto toca actualizar el target_model\n",
    "        self.current_step = 0\n",
    "\n",
    "    def task(self, epsilon, max_iterations=None):\n",
    "        self.current_state, _ = env.reset()\n",
    "        self.current_step = 0\n",
    "\n",
    "        max_iterations = max_iterations or np.inf\n",
    "        reward = 0\n",
    "        running = True\n",
    "        while running and self.current_step < max_iterations:\n",
    "            perception = self.perceive()\n",
    "            action = self.decide(perception, epsilon)\n",
    "            transition = self.act(action)\n",
    "            reward += transition.reward\n",
    "            running = not transition.terminated\n",
    "            self.current_step += 1\n",
    "        return reward\n",
    "\n",
    "    def perceive(self):\n",
    "        return self.current_state\n",
    "\n",
    "    def decide(\n",
    "            self,\n",
    "            perception: np.ndarray,\n",
    "            epsilon: float\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Decide mediante la estrategia ε-greedy que acción ejecutar.\n",
    "\n",
    "        El valor de epsilon controla el equilibrio entre selección aleatoria o\n",
    "        voraz. Cuanto más cercano a 0, más probable será que\n",
    "\n",
    "        :param perception: El estado del entorno que ha percibido el agente.\n",
    "        :param epsilon: La probabilidad de que la selección sea aleatoria.\n",
    "        \"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            # Selección aleatoria\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            # Selección voraz\n",
    "            perception = perception[np.newaxis, ...]\n",
    "            q_values = self.model.predict(perception, verbose=0)[0]\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def act(self, action):\n",
    "        # Ejecutamos la acción\n",
    "        next_state, reward, terminated, _, _ = self.env.step(action)\n",
    "\n",
    "        # Actualizamos el estado en el que nos encontramos ahora\n",
    "        self.current_state = next_state\n",
    "        \n",
    "        # Añadimos a la memoria la transición que acabamos de hacer\n",
    "        self.memory.append(Transition(\n",
    "            prev_state=self.current_state,\n",
    "            next_state=next_state,\n",
    "            action=action,\n",
    "            reward=reward,\n",
    "            terminated=terminated,\n",
    "        ))\n",
    "\n",
    "        # Comprobamos si toca actualizar el modelo objetivo\n",
    "        if self.current_step % self.target_model_update_rate == 0:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        # Comprobamos si toca actualizar el modelo base\n",
    "        if len(self.memory) > self.batch_size and self.current_step % self.train_steps_rate == 0:\n",
    "            # Entrenamos con un batch aleatorio de la memoria del agente\n",
    "            batch = self.memory.batch(n=self.batch_size)\n",
    "            self.model.fit(\n",
    "                x=np.array([t.prev_state for t in batch]),\n",
    "                y=self.compute_labels(batch),\n",
    "                batch_size=self.batch_size,\n",
    "                verbose=0,\n",
    "            )\n",
    "\n",
    "        return self.memory[-1]\n",
    "\n",
    "    def compute_labels(self, transitions):\n",
    "        \"\"\"Para entrenar necesitamos saber cuál es la acción que queremos\n",
    "           predecir. Para ello lo que haremos será mantener activa únicamente\n",
    "           aquella acción que mejor resultado nos da, dejando el resto de\n",
    "           acciones a 0.\"\"\"\n",
    "        # Tomamos los estados destino de las transiciones del batch\n",
    "        next_states = np.array([t.next_state for t in transitions])\n",
    "\n",
    "        # Calculamos los q-values tanto del modelo base como del objetivo\n",
    "        q_base = self.model.predict(next_states, verbose=0)\n",
    "        q_target = self.target_model.predict(next_states, verbose=0)\n",
    "\n",
    "        targets = []\n",
    "        for i, transition in enumerate(transitions):\n",
    "            # Seleccionamos la mejor acción\n",
    "            best_action_base = np.argmax(q_base[i])\n",
    "            best_action_target = q_target[i][best_action_base]\n",
    "\n",
    "            # Calculamos su recompensa asociada\n",
    "            reward = transition.reward\n",
    "            if not transition.terminated:\n",
    "                reward += self.gamma * best_action_target\n",
    "\n",
    "            # Creamos la label de entrenamiento asociada a este ejemplo\n",
    "            target_vector = [0 for _ in range(self.env.action_space.n)]\n",
    "            target_vector[transition.action] = reward\n",
    "            targets.append(target_vector)\n",
    "\n",
    "        return np.array(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a definir un modelo que entrenaremos para que nuestro agente aprenda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuetro modelo será una red neuronal de tipo perceptrón multicapa con $8$ entradas, correspondientes a las 8 observaciones del módulo lunar, y $4$ salidas, correspondientes a las 4 acciones posibles.\n",
    "\n",
    "Sin embargo, vamos a introducir una última modificación (y de verdad que es la última) a la forma en la que hemos estado creando los modelos hasta ahora.\n",
    "\n",
    "Si usamos el error cuadrático medio (MSE) como función de pérdida, es muy probable que los errores durante el entrenamiento sean extremadamente altos. El problema de que aparezcan errores muy altos continuadamente es que existe el riesgo (también muy alto) de que el modelo se sobreajuste, y por tanto de que no llegue a una buena solución. Para ello usaremos la función de pérdida de _Huber_, pero un poco modificada. Veamos cómo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Huber loss_ (un poco a medida)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de pérdida de Huber es bastante menos sensible a los valores atípicos que la MSE porque es exponencial sólo hasta cierto umbral, a partir del cual se convierte en lineal. Esto conduce que la convergencia del proceso de entrenamieto sea más estable.\n",
    "\n",
    "Keras implementa la función de pérdida de Huber (`tf.keras.losses.Huber`), donde se le especifica a partir de qué valor de $x$ (mediante el parámetro `delta`) se empieza a comportar como lineal en lugar de cuadrática.\n",
    "\n",
    "Ahora bien, nuestro modelo toma como entrada un estado $s_t$, y como salida **todos** los valores $Q$ de todas las acciones posibles. Si queremos actualizar el modelo para que aprenda de haber transicionado del estado $s_t$ al estado $s_{t+1}$, sólo querremos calcular el _loss_ de la salida de la acción para dicha transición.\n",
    "\n",
    "Por ello, implementaremos una función un poco diferente que haga esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_huber_loss(delta):\n",
    "    \"\"\"Función de pérdida de huber pero personalizada.\n",
    "\n",
    "    En este caso, la pérdida se calculará sólo para aquellos valores que\n",
    "    sean distintos de 0.\n",
    "\n",
    "    :param delta: Determina el intervalo (-delta, delta) según el cual\n",
    "        todos aquellos valores que quedan dentro del intervalo tienen un\n",
    "        error cuadrático, y fuera un error lineal.\n",
    "    :returns: La función de pérdida configurada para el parámetro delta.\n",
    "    \"\"\"\n",
    "    half = tf.constant(0.5, dtype=tf.keras.backend.floatx())\n",
    "\n",
    "    def f(y, ŷ):\n",
    "        # Máscara para todas aquellas acciones que no son 0 (es decir,\n",
    "        # las que no provocaron la transición)\n",
    "        mask = tf.cast(tf.not_equal(y, 0.0), tf.keras.backend.floatx())\n",
    "        # Error cometido en aquellas acciones válidas (valor absoluto)\n",
    "        error = tf.abs(y - ŷ) * mask\n",
    "        # Dependiendo de delta, calculamos error cuadrático o lineal\n",
    "        loss = tf.where(\n",
    "            error < delta,\n",
    "            half * tf.square(error),  # Error cuadrático\n",
    "            delta * (error - half * delta)  # Error lineal\n",
    "        )\n",
    "        # El loss será la media de todos los valores distintos de cero\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "    f.__name__ = 'custom_huber_loss'\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, supongamos que nuestro agente tiene 4 acciones disponibles. En dicho caso, nuestro modelo podría predecir `[1, 2, 3, 4]` como valores $Q$. Si en la transición de estado sobre la que estamos entrenando tomamos la acción 3 y obtuvimos un valor objetivo de 6, entonces obtenemos un vector objetivo de `[6, 0, 0, 0]`. Nuestra función hará un paso previo de anular las salidas cuyo `y` es `0` para aplicar el error correctamente, usando al final los vectores `y = [1, 0, 0, 0]` e `ŷ = [0, 0, 12, 0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El modelo de nuestro agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora sí que sí, ya tenemos todo para crear nuestro modelo. Crearemos una función que es la que le pasaremos a nuestro agente para que la construya cuando lo necesite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(env: gym.Env) -> tf.keras.models.Model:\n",
    "    \"\"\"Crea un nuevo modelo con nuestro agente.\"\"\"\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(8, activation='relu', input_shape=env.observation_space.shape),\n",
    "        tf.keras.layers.Dense(8, activation='relu'),\n",
    "        tf.keras.layers.Dense(env.action_space.n, activation='linear'),\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss=custom_huber_loss(1))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que nuestro modelo se compila usando la función de pérdida de _Huber_ que hemos definido previamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendiendo a aterrizar el módulo lunar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya tenemos todas las piezas disponibles para que nuestro módulo lunar aprenda. Vamos allá:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para la duración del entrenamiento\n",
    "NUM_TASKS = 10000\n",
    "MAX_STEPS_PER_TASK = 1000\n",
    "\n",
    "# Para la implementación del ε-greedy con desvanecimiento\n",
    "EPSILON_MAX = 1.\n",
    "EPSILON_MIN = 0.01\n",
    "EPSILON_DEC = EPSILON_MAX / NUM_TASKS*100\n",
    "\n",
    "# Ventana de la media móvil (para el plot)\n",
    "RUNNING_AVG_WINDOW_SIZE = 100\n",
    "\n",
    "# Por si queremos usar un modelo previamente salvado\n",
    "def existing_model(task_num):\n",
    "    def f():\n",
    "        \"\"\"Carga un modelo salvado previamente\"\"\"\n",
    "        path = f'tmp/lunar-lander-{task_num}.h5'\n",
    "        return tf.keras.models.load_model(path, custom_objects={\n",
    "            'custom_huber_loss': custom_huber_loss(1)\n",
    "        })\n",
    "    return f\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "agent = Agent(\n",
    "    env=env,\n",
    "    model=build_model,\n",
    "    target_model_update_rate=128,\n",
    "    train_steps_rate=4,\n",
    "    memory_size=2**16,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    ")\n",
    "\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('Task')\n",
    "ax.set_ylabel('Reward')\n",
    "\n",
    "epsilon = EPSILON_MAX\n",
    "rewards = []\n",
    "rewards_avg = []\n",
    "for task in range(NUM_TASKS):\n",
    "    # Reseteamos el entorno y el agente para comenzar un nuevo episodio\n",
    "    reward = agent.task(epsilon=epsilon, max_iterations=MAX_STEPS_PER_TASK)\n",
    "\n",
    "    # Salvamos el modelo\n",
    "    agent.model.save(f\"tmp/lunar-lander-{task}.h5\")\n",
    "\n",
    "    # Actualizamos el valor de epsilon\n",
    "    epsilon = max(EPSILON_MIN, epsilon - EPSILON_DEC)\n",
    "\n",
    "    # Actualizamos el histórico de valores\n",
    "    rewards.append(reward)\n",
    "    if len(rewards) < RUNNING_AVG_WINDOW_SIZE:\n",
    "        rewards_avg.append(np.nan)\n",
    "    else:\n",
    "        rewards_avg.append(np.mean(rewards[-RUNNING_AVG_WINDOW_SIZE:]))\n",
    "\n",
    "    # Pintamos la gráfica\n",
    "    ax.cla()\n",
    "    ax.set_title(f'Task {task} (ε = {epsilon:5.4f}) reward = {reward:5.4f} (best {max(rewards):5.4f} on task {np.argmax(rewards)})')\n",
    "    ax.plot(rewards, linewidth=0.5, label='Instant reward')\n",
    "    ax.plot(rewards_avg, linewidth=1.5, label=f'{RUNNING_AVG_WINDOW_SIZE} steps average')\n",
    "    ax.legend()\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    # HE ACOTADO EL ERROR DEL MEMORY LEAK AL FIT O PREDICT DEL MODELO. POR LO\n",
    "    # VISTO SE VA OCUPANDO MEMORIA POR ALGUNA RAZÓN. A LO MEJOR LOS TENSORES\n",
    "    # QUE SE CREAN PARA EL INPUT SE AÑADEN AL GRAFO Y NUNCA SE LIBERAN, PERO\n",
    "    # NO SÉ CÓMO MIRARLO.\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DESCRIBIR CÓMO HA IDO LA PROGRESIÓN DEL ENTRENAMIENTO (ESPEREMOS QUE BIEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver cómo se comporta el módulo lunar que mejor recompensa ha tenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_TASK = np.argmax(rewards)\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "path = F'tmp/lunar-lander-{BEST_TASK}.h5'\n",
    "model = tf.keras.models.load_model(path, custom_objects={\n",
    "    'custom_huber_loss': custom_huber_loss(1)\n",
    "})\n",
    "\n",
    "state, _ = env.reset()\n",
    "\n",
    "terminated = truncated = False\n",
    "while not (terminated or truncated):\n",
    "    # Seleccionamos una acción dado el actual estado ...\n",
    "    action = np.argmax(model.predict(state[np.newaxis, ...], verbose=0)[0])\n",
    "    # ... y la ejecutamos, obteniendo el nuevo estado\n",
    "    state, _, terminated, truncated, _ = env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESCRIBIR LAS CONCLUSIONES SOBRE EL ENTRENAMIENTO CUANDO NO SE FUNDA LA MEMORIA DE MI MÁQUINA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
