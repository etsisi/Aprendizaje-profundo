{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca9707c3",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "# Transfer Learning<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Authors: Alberto Díaz Álvarez and Félix José Fuentes Hurtado<br>Last update: 2023-03-28</small></i></div> \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ad5ac6",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a8d9ed",
   "metadata": {},
   "source": [
    "In this notebook we are going to work with the concept of \"Transfer Learning\". The idea behind it is \"let's take advantage of the knowledge learned in one model for the training of another model\".\n",
    "\n",
    "Being a bit more specific, the process consists of making use of a neural network previously trained with good performance on a larger data set, using it as a basis on which to create a new model that leverages the accuracy of that previous network for a new task. The \"intuition\" behind this is that, as the first layers deal with certain features (in our example, image features), a problem that also deals with these types of features will use the same or very similar ones (in our example edges, blobs, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a448bd3c",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7442cb2f",
   "metadata": {},
   "source": [
    "We will learn how to save and load models, and partially use them to create models based on others already trained using transfer learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fed507",
   "metadata": {},
   "source": [
    "## Libraries and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7563cf81",
   "metadata": {},
   "source": [
    "Next we will import the libraries that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b77bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emnist\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce86e18",
   "metadata": {},
   "source": [
    "We also set some parameters to adapt the graphic presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bbabf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a0af3a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ba69d",
   "metadata": {},
   "source": [
    "## Feature extraction vs fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1509da9",
   "metadata": {},
   "source": [
    "There are two extremes when using transfer Learning; in one, we start from a pre-trained network, but allow some of the weights to be modified (usually the last layer or layers). This is called \"fine-tuning\" or _fine-tuning_ because we are slightly adjusting the weights of the pre-trained network to the new task. We usually train such a network with a lower learning rate than normal, since we expect that the features are already relatively good and do not need to be changed too much.\n",
    "\n",
    "At the other extreme, it consists of taking the pre-trained network and totally freezing the weights, using one of its hidden layers (usually the last one) as a feature extractor and thus as input to a smaller neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3244537",
   "metadata": {},
   "source": [
    "## Saving and loading our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aba9730",
   "metadata": {},
   "source": [
    "In this example we will first use the neural network that we used in the first exercise to solve the MNIST problem. We are going to save and load it to train it again, but this time with the weights already loaded. This is only a sample, for more information please visit [Save and load Keras models](https://www.tensorflow.org/guide/keras/save_and_serialize?hl=sl), where different save and load alternatives are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db2c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(f'Training shape: {x_train.shape} input, {y_train.shape} output')\n",
    "print(f'Test shape:     {x_test.shape} input, {y_test.shape} output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f98e1",
   "metadata": {},
   "source": [
    "This time we are not going to do _one-hot_ encoding on the output. There is a method for calculating the _loss_ analogous to `categorical_crossentropy` called `sparse_categorical_crossentropy` that essentially does the same thing. The only difference is the format in which the output is represented.\n",
    "\n",
    "If this is in _one-hot_ format, you need categorical_crossentropy, and if it is an integer value, `sparse_categorical_crossentropy`. It has no more. The usage depends entirely on how you load the dataset. One advantage of using the `sparse_categorical_crossentropy` is that it saves memory by using a single integer for a class, rather than a full vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics = ['sparse_categorical_accuracy'])\n",
    "model.summary()\n",
    "history = model.fit(x_train, y_train, epochs=100, batch_size=len(x_train), validation_split=0.1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1a1c7",
   "metadata": {},
   "source": [
    "Let's see the evolution of the training graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde03ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6886a634",
   "metadata": {},
   "source": [
    "Now let's save the model. We will evaluate against the test set before saving and after loading to make sure it is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bbebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the metrics\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Original model: {loss} loss, {accuracy} accuracy')\n",
    "# Save model\n",
    "model.save('tmp/supermodel.h5')\n",
    "# Load model\n",
    "model2 = tf.keras.models.load_model('tmp/supermodel.h5')\n",
    "# Extract the metrics from the loaded model\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Lodaded model:  {loss} loss, {accuracy} accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f4d422",
   "metadata": {},
   "source": [
    "The good thing is that we can now continue to train the model where we left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e026b8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.fit(x_train, y_train, epochs=100, batch_size=len(x_train), validation_split=0.1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1106c0",
   "metadata": {},
   "source": [
    "We can examine the trend of this training phase to see that it does indeed start where it left off in the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66edd5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee8fbe9",
   "metadata": {},
   "source": [
    "Now, we are going to use our model to try to recognize not only numbers, but also letters (a bit pretentious, yes, but it is to learn how to use our models with _transfer learning_). For this we will rely on the set [https://www.nist.gov/itl/products-and-services/emnist-dataset](EMNIST (Extended MNIST)) and a _wrapper_ called `emnist` (`pip install emnist`) to avoid having to download the dataset by hand.\n",
    "\n",
    "Once we have it installed, we will use the _dataset_ with the balanced classes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1284a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_emnist, y_train_emnist = emnist.extract_training_samples('balanced')\n",
    "x_test_emnist, y_test_emnist = emnist.extract_test_samples('balanced')\n",
    "\n",
    "x_train_emnist = x_train_emnist / 255\n",
    "x_test_emnist = x_test_emnist / 255\n",
    "\n",
    "print(f'Training shape: {x_train_emnist.shape} input, {y_train_emnist.shape} output')\n",
    "print(f'Test shape:     {x_test_emnist.shape}  input, {y_test_emnist.shape} output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feed30a",
   "metadata": {},
   "source": [
    "The examples have the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa07a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train_emnist[0], cmap='hot');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b64b0d",
   "metadata": {},
   "source": [
    "And their corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ecbea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_emnist[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc68c1b",
   "metadata": {},
   "source": [
    "What we will do is load our previously saved model and use its first layers without modification. We will only change the last layer to classify our examples, which are quite a few more.\n",
    "\n",
    "This will be the only layer we will train. This is done with the assumption that the first layers of a model extract or learn the relevant features that make the examples unique, and that the last ones learn to infer from these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a952f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('tmp/supermodel.h5')\n",
    "\n",
    "model_emnist = tf.keras.Sequential()\n",
    "for i, layer in enumerate(model.layers[:-1]):  # ¡¡No incluimos la última!!\n",
    "    model_emnist.add(layer)\n",
    "    model_emnist.layers[-1].trainable = False  # Si no, entrenará los parámetros de estas capas\n",
    "model_emnist.add(tf.keras.layers.Dense(47, activation='softmax'))\n",
    "\n",
    "model_emnist.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics = ['sparse_categorical_accuracy'])\n",
    "model_emnist.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a800ab7",
   "metadata": {},
   "source": [
    "Looking at the summary of the network architecture, we can see that of all the parameters that exist, only 235 will be trained, those corresponding to the connections between the penultimate and final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_emnist.fit(\n",
    "    x_train_emnist, y_train_emnist, epochs=100, batch_size=len(x_train_emnist), validation_split=0.1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2972e90",
   "metadata": {},
   "source": [
    "Now let's see how the training evolution has progressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c26a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c56084",
   "metadata": {},
   "source": [
    "Well, maybe it was not the best example, since the starting model is small and not very generalist. But at least we know how to manipulate a model to create a new one from a previously trained one. Now we will use another larger model to see how it behaves with the _dataset_ EMNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b297af",
   "metadata": {},
   "source": [
    "### Using a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3991b9b9",
   "metadata": {},
   "source": [
    "We can make use of saved models as part of new models. It is usually not trivial, but not too complicated as the models themselves usually provide help or at least the architecture description to understand how to make them.\n",
    "\n",
    "We have multiple models to work with. In Keras alone there are dozens of pre-trained models ready to download from the `applications` API. Let's see an example with the _ResNet50_ model.\n",
    "\n",
    "For this, we will need to transform our EMNIST images (actually two-dimensional arrays) into 3-channel color images. Also the minimum expected image size is $75 \\times 75$, so we will have to adjust them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d58231",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_emnist.reshape((-1, 28, 28, 1))\n",
    "x_test = x_test_emnist.reshape((-1, 28, 28, 1))\n",
    "x_train, x_test = tf.image.resize(x_train, (32, 32)), tf.image.resize(x_test, (32, 32))\n",
    "x_train, x_test = tf.image.grayscale_to_rgb(x_train), tf.image.grayscale_to_rgb(x_test)\n",
    "y_train, y_test = y_train_emnist, y_test_emnist\n",
    "\n",
    "print(f'Training shape: {x_train.shape} input, {y_train.shape} output')\n",
    "print(f'Test shape:     {x_test.shape}  input, {y_test.shape} output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2fd8a",
   "metadata": {},
   "source": [
    "Now, as we have done before, we will load the ResNet50 model without including the last layer (argument `include_top` to `False`). We will also specify that we **do not** want to train the preloaded model. Microsoft engineers have spent time and machines so that these models are quite well trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67af95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = tf.keras.applications.ResNet50(input_shape=[32, 32, 3], include_top=False)\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    pretrained_model,\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(47, activation='softmax')\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics = ['sparse_categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f0423",
   "metadata": {},
   "source": [
    "Now all that would be left for us to do is to train. Fortunately, out of more than 23 million parameters, we will only adjust a little more than 96,000, so great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, batch_size=1024, validation_split=0.1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc08b54",
   "metadata": {},
   "source": [
    "Slow, huh? Well, although we have avoided training millions of parameters, what we have not been able to avoid is inference, and this usually costs.\n",
    "\n",
    "Let's see how the training has evolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53cff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05239ef4",
   "metadata": {},
   "source": [
    "## Where to obtain pre-trained models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f50fc9",
   "metadata": {},
   "source": [
    "As of today (March 28, 2023) there are more than $38$ pretrained models available in Keras through the `applications` API. When downloaded, the weights will automatically download into the `~/.keras/models/` directory. Unfortunately, all API models to date are used for images.\n",
    "\n",
    "We have however more sources of pre-trained models available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f368784e",
   "metadata": {},
   "source": [
    "### [TensorFlow Hub](https://tfhub.dev/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa685084",
   "metadata": {},
   "source": [
    "As it could not be otherwise, there is a \"Hub\" for TensorFlow models. An example of instantiation can be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f20e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    hub.KerasLayer(\n",
    "        'https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4',\n",
    "        input_shape=(224, 224, 3),\n",
    "        trainable=False),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6817a8",
   "metadata": {},
   "source": [
    "The TensorFlow Hub API is available via pip: `pip install tensorflow-hub`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23811760",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1269f77",
   "metadata": {},
   "source": [
    "We will see them later in Natural Language Processing (NLP), but for the record, there are some independent projects famous enough to have their own download site. This is the case of _embeddings_, for example:\n",
    "\n",
    "* [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)\n",
    "* [Word2vec](https://code.google.com/archive/p/word2vec/)\n",
    "* [fastText](https://fasttext.cc/docs/en/english-vectors.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202644c",
   "metadata": {},
   "source": [
    "### [Hugging face](https://huggingface.co/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c0acb",
   "metadata": {},
   "source": [
    "Hugging Face es una empresa con sede en los Estados Unidos que desarrolla herramientas para la creación de aplicaciones basadas en aprendizaje automático. Son los desarrolladores de la biblioteca `transformers`, la cual se usa extensamente para aplicaciones de NLP.\n",
    "\n",
    "Su plataforma permite a los usuarios compartir modelos y conjuntos de datos de aprendizaje automático. En la actualidad disponen de miles de modelo preentrenados para realizar tareas de todo tipo:\n",
    "\n",
    "- **Visión artificial**, como clasificación de imágenes o de vídeo, segmentación o detección de objetos.\n",
    "- **NLP**, como análisis de sentimiento, generación de texto o traducción.\n",
    "- **Audio**, como reconocimiento del habla, clasificación o generación de audio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9ad2d7",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Back to top](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
