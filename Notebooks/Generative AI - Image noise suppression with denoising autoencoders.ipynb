{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4480ff39",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Image noise suppression with denoising autoencoders<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Autor: Alberto Díaz Álvarez<br>Última actualización: 2023-04-27</small></i></div>\n",
    "                                                  \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2489222",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17030ba",
   "metadata": {},
   "source": [
    "Denoising autoencoders (first presented on [Extracting and Composing Robust Features with Denoising Autoencoders](https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf)) are a type of neural network whosepurpose is to remove noise from input data so that it can be used more effectively in downstream tasks. Noise can be introduced into data in many ways, such as in images where noise can be caused by factors like lighting conditions, sensor errors or compression artifacts. Removing this noise can lead to better performance in tasks such as image recognition or classification.\n",
    "\n",
    "| <img src=\"https://production-media.paperswithcode.com/methods/Denoising-Autoencoder_qm5AOQM.png\" alt=\"Denoising autoencoder\" width=\"50%\"> | \n",
    "|:--:| \n",
    "| *A denoising autoencoder is purposely trained with noise-corrupted data to learn how to remove it effectively. Source: Kumar, V., Nandi, G. C., & Kala, R. (2014, August). [Static hand gesture recognition using stacked denoising sparse autoencoders](https://ieeexplore.ieee.org/document/6897155). In 2014 7th IC3 (pp. 99-104). IEEE.]( (last visited April 24, 2023).* |\n",
    "\n",
    "Denoising autoencoders work by training a neural network to learn a compressed representation of the input data, which is then used to reconstruct the original data but without the noise. This process involves adding artificial noise to the input data and training the neural network to reconstruct the original data without the noise. The network is trained to identify patterns in the input data that are consistent across the different examples of the same class and to ignore the patterns that are not consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d1e17",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9e324",
   "metadata": {},
   "source": [
    "Our goals are to understand what a denoising autoencoder is, build a denoising autoencoder using Keras, train the denoising autoencoder on the Fashion MNIST dataset and generate denoised images using the trained autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d401eac",
   "metadata": {},
   "source": [
    "## Libraries and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa528aec",
   "metadata": {},
   "source": [
    "Next we will import the libraries that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e97df",
   "metadata": {},
   "source": [
    "We will also configure some parameters to adapt the graphical presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca72f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fec89",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7223928",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d3a3bc",
   "metadata": {},
   "source": [
    "We'll use the Fashion MNIST datasetin the same way we did on the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07075092",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255, x_test / 255\n",
    "\n",
    "print(f'Training shape: {x_train.shape} input')\n",
    "print(f'Test shape:     {x_test.shape} input')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9cb29",
   "metadata": {},
   "source": [
    "## A noise layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbc803a",
   "metadata": {},
   "source": [
    "Here is a possible implementation of a custom layer that adds Gaussian noise to its input to output the same input but messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd6cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoiseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, μ=.0, σ=1., *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.μ = μ\n",
    "        self.σ = σ\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if training:\n",
    "            x += tf.random.normal(x.shape, mean=self.μ, stddev=self.σ)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ae18c",
   "metadata": {},
   "source": [
    "Ok, in Keras we already have an implementation for adding gaussian noise (yet always centered on 0, so our layer is more generic), the `tf.keras.layers.GaussianNoise` layer, although this implementation allows us to see an argument that we had not seen before. If we look at it, the layer **only acts when we are training**, and does nothing if the mode is \"not training\" (i.e. inferring). Ya que la hemos implementado, vamos a usarla para añadir ruido en lugar de la existente en la librería."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded7d059",
   "metadata": {},
   "source": [
    "## Denoising autoencoder implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bfb0d5",
   "metadata": {},
   "source": [
    "Now, let's go to the implementation of the denoising autoencoder, which we will see that it is very similar to the autoencoders we have seen so far. We will use as a baseline the vanilla autoencoder, and we will build our denoising autoencoder from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc7e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAutoencoder(tf.keras.models.Model):\n",
    "    \"\"\"Represents a simple denoiseing autoencoder\"\"\"\n",
    "    def __init__(self, input_dim, latent_dim, noise_mean=.0, noise_stddev=1.):\n",
    "        super().__init__()\n",
    "\n",
    "        flatten_dim = None\n",
    "        if isinstance(input_dim, (list, tuple)):\n",
    "            flatten_dim = math.prod(input_dim)\n",
    "        elif isinstance(input_dim, int):\n",
    "            flatten_dim = input_dim\n",
    "            input_dim = (input_dim,)\n",
    "        else:\n",
    "            raise ValueError('Argument input_dim must be a tuple or an int')\n",
    "        \n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(),\n",
    "            GaussianNoiseLayer(μ=noise_mean, σ=noise_stddev),\n",
    "            tf.keras.layers.Dense(latent_dim, activation=tf.keras.layers.LeakyReLU()),\n",
    "        ])\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(flatten_dim, activation='sigmoid'),\n",
    "            tf.keras.layers.Reshape(input_dim)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f08b289",
   "metadata": {},
   "source": [
    "We will create a new denoising autoencoder with a considerable some of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b287bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dae = DenoisingAutoencoder(input_dim=(28, 28), latent_dim=256, noise_mean=.0, noise_stddev=2.)\n",
    "dae.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfb1975",
   "metadata": {},
   "source": [
    "And we will train him to see how he behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43207586",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = dae.fit(x_train, x_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c1d328",
   "metadata": {},
   "source": [
    "How good has the training been?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0554c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65d4f7",
   "metadata": {},
   "source": [
    "Well, good in a way, although it could have been much better. Let's look at some coding/decoding examples from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda8231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "images = np.array(random.sample(list(x_train), n))\n",
    "\n",
    "encoded = dae.encoder(images).numpy()\n",
    "decoded = dae.decoder(encoded).numpy()\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title('Original')\n",
    "\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded[i])\n",
    "    plt.title('Reconstructed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c5d8ae",
   "metadata": {},
   "source": [
    "And with data you have never seen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8445eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(random.sample(list(x_test), n))\n",
    "encoded = dae.encoder(images).numpy()\n",
    "decoded = dae.decoder(encoded).numpy()\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title('Original')\n",
    "\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded[i])\n",
    "    plt.title('Reconstructed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9bf36",
   "metadata": {},
   "source": [
    "He doesn't seem to be doing too badly. Let's see now how it behaves by adding a lot of noise to the images during the training.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a27bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dae = DenoisingAutoencoder(input_dim=(28, 28), latent_dim=192, noise_mean=.0, noise_stddev=6.)\n",
    "dae.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d0ab75",
   "metadata": {},
   "source": [
    "Let us now train our model with the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0daf891",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = dae.fit(x_train, x_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a8ffb",
   "metadata": {},
   "source": [
    "Let's see how the training has evolved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5833d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb4f2b2",
   "metadata": {},
   "source": [
    "The training doesn't seem to be very good. Let's see how some examples of the training set are encoded and decoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "images = np.array(random.sample(list(x_train), n))\n",
    "\n",
    "encoded = dae.encoder(images).numpy()\n",
    "decoded = dae.decoder(encoded).numpy()\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title('Original')\n",
    "\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded[i])\n",
    "    plt.title('Reconstructed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9944de34",
   "metadata": {},
   "source": [
    "Now let's see with data you have never seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(random.sample(list(x_test), n))\n",
    "encoded = dae.encoder(images).numpy()\n",
    "decoded = dae.decoder(encoded).numpy()\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title('Original')\n",
    "\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded[i])\n",
    "    plt.title('Reconstructed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43585490",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7defd88",
   "metadata": {},
   "source": [
    "Denoising autoencoders have a wide range of applications in computer vision, including image denoising, inpainting, and super-resolution (although we have only seen its application here on a very small toy data set). They can also be applied to other types of data, such as audio and text.\n",
    "\n",
    "One advantage of denoising autoencoders is that they are self-supervised, which means that they don't require labeled data to train. This can be particularly useful in cases where labeled data is scarce or expensive to obtain.\n",
    "\n",
    "However, there are some limitations to denoising autoencoders. They may not be effective in cases where the noise is too severe or where the noise pattern is complex and difficult to model. Additionally, denoising autoencoders are not always able to preserve fine details in the input data, especially if the noise is highly correlated with these details.\n",
    "\n",
    "In summary, denoising autoencoders are a powerful tool for removing noise from input data and can be applied to a wide range of applications in computer vision and other fields. However, their effectiveness depends on the nature and severity of the noise and the complexity of the noise pattern, as well as the specific details of the data being processed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33005bde",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
